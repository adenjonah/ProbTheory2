\documentclass[9pt,landscape,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.4in]{geometry}
\usepackage{multicol}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}

% Better text handling - prevent overflow
\tolerance=9999
\emergencystretch=3em
\hyphenpenalty=10000
\sloppy
\setlength{\columnsep}{0.2in}

% Custom commands for consistent formatting
\newcommand{\concept}[1]{\textbf{#1}}
\newcommand{\formula}[1]{\boxed{#1}}
\newcommand{\note}[1]{\textit{Note: #1}}
\newcommand{\warning}[1]{\textcolor{red}{\textbf{! #1}}}
\newcommand{\finance}[1]{\textcolor{blue}{\textbf{[\$] #1}}}
\newcommand{\postmidterm}[1]{\textcolor{orange}{\textbf{* #1}}}
\newcommand{\quickref}[2]{\textbf{#1:} #2}

% Define colors
\definecolor{sectioncolor}{RGB}{0,102,204}
\definecolor{formulacolor}{RGB}{0,153,0}

\begin{document}

\begin{center}
{\Huge \textbf{Probability Theory Final Exam Cheat Sheet}}\\
\vspace{0.2cm}
{\large December 16, 2025 | 7:10pm-8:40pm | 3 Questions, 1.5 Hours}\\
{\small \textit{Open Book Exam - Focus on Post-Midterm 2 Material}}
\end{center}

\begin{multicols}{3}

% ============================================================
\section*{\textcolor{sectioncolor}{0. QUICK REFERENCE GUIDE}}
% ============================================================

\subsection*{TERMINOLOGY TRAPS (CRITICAL!)}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item \textbf{``Gaussian''} = \textbf{Normal}! N($\mu$,$\sigma^2$)
\item \textbf{``Gaussian vector''} = \textbf{MVN} (Multivariate Normal)
\item \textbf{``Independent components''} = $\rho = 0$ (for MVN: independence!)
\item \textbf{``Mean $\theta = 3$'' (Exp)} $\Rightarrow$ $\lambda = 1/3$ NOT 3!
\item For MVN ONLY: $\rho = 0 \Leftrightarrow$ independent
\item $\psi(t)$ = MGF (professor's notation)
\end{itemize}

\subsection*{VISUAL DECISION TREE (Start Here!)}
{\scriptsize
\begin{verbatim}
START: What type of problem?
  |
  +--[Named Distribution?]
  |    +--"Gaussian/Normal" -> Sec 3.3 (single) or 4.5 (joint)
  |    +--"Exponential" -> Sec 3.4 [lambda=1/mean!]
  |    +--"Poisson" -> Sec 2.3
  |    +--"Binomial" -> Sec 2.2
  |    +--"Lognormal" -> Sec 7.3 (stock prices!)
  |    +--"Beta" -> Sec 3.6 (priors!)
  |
  +--[Multiple Variables?]
  |    +--"Joint PDF/PMF" -> Sec 4.1
  |    +--"Bivariate Normal/Gaussian vector" -> Sec 4.5
  |    +--"X+Y, sum" -> MGF (5.2) or direct
  |    +--"Max/Min" -> Order Stats (4.7)
  |    +--"X|Y=y" -> Conditional (4.2, 4.5)
  |
  +--[Approximation/Limits?]
  |    +--"Large n/approximate" -> CLT (6.1)
  |    +--"Sample mean" -> CLT (6.1)
  |    +--"n games/trials" -> CLT (6.1) Template B
  |
  +--[Bayesian?]
  |    +--"Prior/Posterior" -> Sec 7.2
  |    +--"Update belief" -> Bayes (1.3, 7.2)
  |    +--"Defective rate" -> Discrete Bayes (8.8)
  |    +--"Monty Hall" -> Template J
  |
  +--[Expectation?]
       +--"E[X|Y]" -> Cond. Expectation (7.1)
       +--"Total Expectation" -> E[X]=E[E[X|Y]]
\end{verbatim}
}

\subsection*{Emergency Quick Reference: Problem Phrase $\rightarrow$ Section}
{\small
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
\item \textbf{``Gaussian''} $\rightarrow$ Normal! Sec 3.3
\item \textbf{``Gaussian vector''} $\rightarrow$ MVN! Sec 4.5
\item \textbf{``Independent components''} $\rightarrow$ $\rho=0$ for MVN, Sec 4.5
\item \textbf{``Large n'' / ``Approximate''} $\rightarrow$ CLT, Sec 6.1
\item \textbf{``i.i.d.''} $\rightarrow$ Independence, maybe CLT
\item \textbf{``Prior/Posterior''} $\rightarrow$ Bayesian, Sec 7.2
\item \textbf{``Update belief''} $\rightarrow$ Bayes' Theorem, Sec 1.3
\item \textbf{``Conjugate''} $\rightarrow$ Beta-Binomial, Sec 7.2
\item \textbf{``Stock price'' / ``$S_0 e^Z$''} $\rightarrow$ Lognormal, Sec 7.3
\item \textbf{``Mean $\theta$'' (Exp)} $\rightarrow$ $\lambda = 1/\theta$! Sec 3.4
\item \textbf{``Memoryless''} $\rightarrow$ Exponential, Sec 3.4
\item \textbf{``Arrival/Counting process''} $\rightarrow$ Poisson, Sec 2.3
\item \textbf{``Max/Min of n''} $\rightarrow$ Order Statistics, Sec 4.7
\item \textbf{``$\psi(t)$''} $\rightarrow$ MGF! Sec 5.1
\item \textbf{``Conditional distribution''} $\rightarrow$ Sec 4.2, 4.5
\item \textbf{``$E[X|Y]$''} $\rightarrow$ Conditional Expectation, Sec 7.1
\item \textbf{``Total winnings/games''} $\rightarrow$ CLT, Template B
\item \textbf{``Monty Hall''} $\rightarrow$ Bayesian, Sec 8.1
\item \textbf{``Defective rate''} $\rightarrow$ Bayesian, Sec 8.8
\end{itemize}
}

\subsection*{Top 20 Critical Formulas}
\begin{enumerate}[leftmargin=*,itemsep=0pt,topsep=0pt]
\item \formula{P(A|B) = \frac{P(A \cap B)}{P(B)}}
\item \formula{P(H|E) = \frac{P(E|H)P(H)}{P(E)}} (Bayes)
\item \formula{P(A) = \sum P(A|B_i)P(B_i)} (Total Prob)
\item \formula{E[X] = \sum x P(X=x)} (Discrete)
\item \formula{E[X] = \int x f(x)dx} (Continuous)
\item \formula{\text{Var}(X) = E[X^2] - (E[X])^2}
\item \formula{\text{Cov}(X,Y) = E[XY] - E[X]E[Y]}
\item \formula{\rho = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}}
\item \formula{P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}} (Binomial)
\item \formula{P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}} (Poisson)
\item \formula{f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}} (Normal)
\item \formula{Z = \frac{X-\mu}{\sigma}} (Standardization)
\item \formula{M(t) = E[e^{tX}]} (MGF)
\item \formula{E[X] = E[E[X|Y]]} (Total Expectation)
\item \formula{\text{Var}(X) = E[\text{Var}(X|Y)] + \text{Var}(E[X|Y])}
\item \formula{Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)} (CLT)
\item \formula{\text{CI}: \bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}}
\item \formula{\pi(\theta|x) \propto L(x|\theta)\pi(\theta)} (Bayes)
\item \formula{E[e^X] = e^{\mu + \sigma^2/2}} (Lognormal)
\item \formula{f_{UV}(u,v) = f_{XY}(x,y)|J|} (Jacobian)
\end{enumerate}

\subsection*{Common Mistakes Checklist}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item[$\square$] Forgot continuity correction for discrete$\rightarrow$normal
\item[$\square$] Confused $P(A|B)$ with $P(B|A)$
\item[$\square$] Didn't check independence before using formulas
\item[$\square$] Wrong integration limits for marginals
\item[$\square$] Forgot to normalize Bayesian posterior
\item[$\square$] Used Binomial instead of Hypergeometric
\item[$\square$] Forgot absolute value of Jacobian
\item[$\square$] Assumed correlation implies causation
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{1. FUNDAMENTAL CONCEPTS}}
% ============================================================

\subsection*{1.1 Probability Axioms}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} A probability measure satisfies:
  \begin{enumerate}
  \item Normalization: $P(S) = 1$
  \item Non-negativity: $P(A) \geq 0$
  \item Additivity: $P(A \cup B) = P(A) + P(B)$ if $A \cap B = \emptyset$
  \end{enumerate}
\item \concept{Key Formula:} \formula{P(A \cup B) = P(A) + P(B) - P(A \cap B)}
\item \concept{When to Use:} Basic probability calculations
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Identify sample space $S$
  \item Count favorable outcomes
  \item Apply formula
  \end{enumerate}
\item \concept{Example:} Two dice: $P(\text{sum}=7) = 6/36 = 1/6$
\item \concept{Common Pitfalls:} Forgetting the intersection term
\item \note{Equally likely: $P(A) = |A|/|S|$}
\end{itemize}

\subsection*{1.2 Conditional Probability}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Probability of $A$ given $B$ occurred
\item \concept{Key Formula:} \formula{P(A|B) = \frac{P(A \cap B)}{P(B)}, \text{ if } P(B) > 0}
\item \concept{When to Use:} "Given that", "if we know", "conditional on"
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Identify condition $B$ and target $A$
  \item Find $P(A \cap B)$ and $P(B)$
  \item Apply formula
  \end{enumerate}
\item \concept{Example:} Roll dice, sum odd. $P(\text{sum}<8|\text{odd}) = 2/3$
\item \concept{Common Pitfalls:} Confusing $P(A|B)$ with $P(B|A)$
\item \note{Multiplication Rule: $P(A \cap B) = P(B)P(A|B)$}
\end{itemize}

\subsection*{1.3 Bayes' Theorem}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Update probability given evidence
\item \concept{Key Formula:} \formula{P(H_i|E) = \frac{P(E|H_i)P(H_i)}{\sum_j P(E|H_j)P(H_j)}}
\item \concept{When to Use:} "Update", "posterior", "given evidence"
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item List hypotheses $H_i$ with priors $P(H_i)$
  \item Find likelihoods $P(E|H_i)$
  \item Apply Bayes' formula
  \item Normalize if needed
  \end{enumerate}
\item \concept{Example:} Monty Hall: Switch wins 2/3 of time
\item \concept{Common Pitfalls:} Wrong likelihood, forgetting to normalize
\item \note{Total Probability: $P(A) = \sum P(A|B_i)P(B_i)$}
\end{itemize}

\subsection*{1.4 Independence}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} $A$ and $B$ independent if $P(A \cap B) = P(A)P(B)$
\item \concept{Key Formula:} \formula{P(A|B) = P(A) \text{ iff independent}}
\item \concept{When to Use:} Testing if events affect each other
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Calculate $P(A)$, $P(B)$, $P(A \cap B)$
  \item Check if $P(A \cap B) = P(A) \cdot P(B)$
  \item State conclusion
  \end{enumerate}
\item \concept{Example:} Card draws with replacement are independent
\item \concept{Common Pitfalls:} Assuming independence without checking
\item \note{Pairwise $\neq$ Mutual independence}
\end{itemize}

\subsection*{1.5 Counting Methods}
\begin{itemize}[leftmargin=*]
\item \concept{Permutations:} Order matters
  \formula{P(n,k) = \frac{n!}{(n-k)!}}
\item \concept{Combinations:} Order doesn't matter
  \formula{C(n,k) = \binom{n}{k} = \frac{n!}{k!(n-k)!}}
\item \concept{Multinomial:} Multiple categories
  \formula{\frac{n!}{n_1!n_2!\cdots n_k!}}
\item \concept{When to Use:} "How many ways", "arrangements", "selections"
\item \concept{Example:} 6-card poker hands from 52 cards: $\binom{52}{6}$
\item \note{With replacement: $n^k$; Without: $P(n,k)$ or $C(n,k)$}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{2. DISCRETE RANDOM VARIABLES}}
% ============================================================

\subsection*{2.1 PMF and CDF}
\begin{itemize}[leftmargin=*]
\item \concept{PMF:} $p(x) = P(X = x)$, where $\sum p(x) = 1$
\item \concept{CDF:} $F(x) = P(X \leq x) = \sum_{k \leq x} p(k)$
\item \concept{Expectation:} \formula{E[X] = \sum x \cdot P(X=x)}
\item \concept{Variance:} \formula{\text{Var}(X) = E[X^2] - (E[X])^2}
\item \concept{Properties:} CDF is right-continuous, non-decreasing
\item \note{$P(a < X \leq b) = F(b) - F(a)$}
\end{itemize}

\subsection*{2.2 Binomial Distribution (a.k.a. Binomial($n,p$), ``n trials'')}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``n trials'', ``success/failure'', ``fixed number of trials''
\item \concept{Definition:} Number of successes in $n$ independent trials
\item \concept{PMF:} \formula{P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}}
\item \concept{Mean:} $E[X] = np$
\item \concept{Variance:} $\text{Var}(X) = np(1-p)$
\item \concept{MGF:} $M(t) = (1-p+pe^t)^n$
\item \concept{When to Use:} Fixed $n$, constant $p$, independent trials
\item \concept{Example:} Flip coin 10 times, $P(X=6)$ heads with $p=0.5$
\item \warning{Check conditions before using!}
\item \note{Normal approximation when $np(1-p) > 10$}
\end{itemize}

\subsection*{2.3 Poisson Distribution (a.k.a. Poisson($\lambda$), Counting Process)}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``arrival process'', ``counting process'', ``rare events'', ``rate $\lambda$''
\item \concept{Definition:} Count of rare events in fixed interval
\item \concept{PMF:} \formula{P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}}
\item \concept{Mean:} $E[X] = \lambda$
\item \concept{Variance:} $\text{Var}(X) = \lambda$
\item \concept{MGF:} $M(t) = e^{\lambda(e^t-1)}$
\item \concept{When to Use:} Rate $\lambda$ per unit time/space
\item \concept{Example:} Arrivals per hour, defects per batch
\item \concept{Property:} Sum of independent Poissons is Poisson
\item \note{Approximates Binomial when $n$ large, $p$ small, $np = \lambda$}
\end{itemize}

\subsection*{2.4 Geometric Distribution (a.k.a. ``First success'', Memoryless Discrete)}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``first success'', ``waiting for success'', ``trials until success''
\item \concept{Definition:} Number of failures before first success
\item \concept{PMF:} \formula{P(X=k) = p(1-p)^k, \quad k=0,1,2,...}
\item \concept{Mean:} $E[X] = (1-p)/p$
\item \concept{Variance:} $\text{Var}(X) = (1-p)/p^2$
\item \concept{Memoryless:} $P(X=m+n|X \geq m) = P(X=n)$
\item \concept{When to Use:} "First success", "waiting time"
\item \concept{Example:} Roll die until first 6 appears
\item \note{Alternative parameterization: trials until success}
\end{itemize}

\subsection*{2.5 Negative Binomial}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Failures before $r$-th success
\item \concept{PMF:} \formula{P(X=k) = \binom{k+r-1}{k}p^r(1-p)^k}
\item \concept{Mean:} $E[X] = r(1-p)/p$
\item \concept{Variance:} $\text{Var}(X) = r(1-p)/p^2$
\item \concept{When to Use:} "$r$-th success", extended geometric
\item \note{Geometric is special case with $r=1$}
\end{itemize}

\subsection*{2.6 Hypergeometric Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Sampling without replacement
\item \concept{PMF:} \formula{P(X=k) = \frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}}
\item \concept{Parameters:} $N$ total, $K$ success, $n$ sample, $k$ observed
\item \concept{Mean:} $E[X] = n \cdot K/N$
\item \concept{When to Use:} Finite population, no replacement
\item \concept{Example:} Draw 5 cards, probability of 3 aces
\item \warning{Different from Binomial (with replacement)}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{3. CONTINUOUS RANDOM VARIABLES}}
% ============================================================

\subsection*{3.1 PDF and CDF}
\begin{itemize}[leftmargin=*]
\item \concept{PDF:} $f(x) \geq 0$, $\int_{-\infty}^{\infty} f(x)dx = 1$
\item \concept{CDF:} \formula{F(x) = P(X \leq x) = \int_{-\infty}^x f(t)dt}
\item \concept{Probability:} \formula{P(a < X < b) = \int_a^b f(x)dx}
\item \concept{Expectation:} \formula{E[X] = \int_{-\infty}^{\infty} x f(x)dx}
\item \concept{Variance:} \formula{\text{Var}(X) = \int (x-\mu)^2 f(x)dx}
\item \concept{Relation:} $f(x) = F'(x)$ where derivative exists
\item \note{$P(X = a) = 0$ for continuous RV}
\end{itemize}

\subsection*{3.2 Uniform Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Equally likely over interval $[a,b]$
\item \concept{PDF:} \formula{f(x) = \frac{1}{b-a}, \quad a \leq x \leq b}
\item \concept{CDF:} $F(x) = \frac{x-a}{b-a}$ for $a \leq x \leq b$
\item \concept{Mean:} $E[X] = \frac{a+b}{2}$
\item \concept{Variance:} $\text{Var}(X) = \frac{(b-a)^2}{12}$
\item \concept{When to Use:} "Equally likely", "random point"
\item \concept{Example:} Random number between 0 and 1
\item \note{Probability proportional to interval length}
\end{itemize}

\subsection*{3.3 Normal Distribution (a.k.a. Gaussian, N($\mu$,$\sigma^2$))} \postmidterm{High Priority!}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} \textbf{Gaussian} = \textbf{Normal} = N($\mu$,$\sigma^2$) = ``bell curve''
\item \concept{Definition:} Bell curve, most important continuous distribution
\item \concept{PDF:} \formula{f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}}
\item \concept{Notation:} $X \sim N(\mu, \sigma^2)$
\item \concept{Standardization:} \formula{Z = \frac{X-\mu}{\sigma} \sim N(0,1)}
\item \concept{Properties:}
  \begin{itemize}
  \item Linear combination: $aX+b \sim N(a\mu+b, a^2\sigma^2)$
  \item Sum of normals: normal
  \item 68-95-99.7 rule for $\pm 1,2,3$ std dev
  \end{itemize}
\item \concept{MGF:} $M(t) = e^{\mu t + \sigma^2 t^2/2}$
\item \concept{Example:} Heights, measurement errors, CLT limit
\item \note{Use $\Phi(z)$ table for standard normal CDF}
\end{itemize}

\subsection*{3.4 Exponential Distribution (a.k.a. Exp($\lambda$), Memoryless)}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} Exp($\lambda$), ``waiting time'', ``memoryless'', ``inter-arrival time''
\item \warning{``Mean $\theta=3$'' means $\lambda = 1/3$ NOT $\lambda=3$!}
\item \concept{Definition:} Waiting time until event
\item \concept{PDF:} \formula{f(x) = \lambda e^{-\lambda x}, \quad x > 0}
\item \concept{CDF:} $F(x) = 1 - e^{-\lambda x}$
\item \concept{Mean:} $E[X] = 1/\lambda$
\item \concept{Variance:} $\text{Var}(X) = 1/\lambda^2$
\item \concept{Memoryless:} \formula{P(X > s+t | X > s) = P(X > t)}
\item \concept{MGF:} $M(t) = \frac{\lambda}{\lambda - t}$ for $t < \lambda$
\item \concept{When to Use:} Time between Poisson events
\item \concept{Example:} Service times, component lifetime
\item \note{Min of exponentials is exponential}
\end{itemize}

\subsection*{3.5 Gamma Distribution (a.k.a. Gamma($r,\lambda$), Erlang)}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``sum of exponentials'', ``time until $r$-th event'', Erlang (integer $r$)
\item \concept{Definition:} Sum of exponentials, generalization
\item \concept{PDF:} \formula{f(x) = \frac{\lambda^r}{\Gamma(r)}x^{r-1}e^{-\lambda x}, \quad x > 0}
\item \concept{Mean:} $E[X] = r/\lambda$
\item \concept{Variance:} $\text{Var}(X) = r/\lambda^2$
\item \concept{Special Cases:}
  \begin{itemize}
  \item $r=1$: Exponential($\lambda$)
  \item $r=n/2, \lambda=1/2$: Chi-square with $n$ df
  \end{itemize}
\item \concept{When to Use:} Time until $r$-th event
\item \note{$\Gamma(n) = (n-1)!$ for integer $n$}
\end{itemize}

\subsection*{3.6 Beta Distribution (a.k.a. Beta($\alpha,\beta$), Conjugate Prior)}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``prior for probability'', ``proportion model'', ``conjugate to Binomial''
\item \concept{Definition:} Models probabilities/proportions
\item \concept{PDF:} \formula{f(x) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}}
\item \concept{Support:} $0 < x < 1$
\item \concept{Mean:} $E[X] = \frac{\alpha}{\alpha+\beta}$
\item \concept{Variance:} $\text{Var}(X) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$
\item \concept{Special Cases:}
  \begin{itemize}
  \item $\alpha=\beta=1$: Uniform(0,1)
  \item Conjugate prior for Binomial
  \end{itemize}
\item \finance{Used in Bayesian statistics}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{4. MULTIVARIATE DISTRIBUTIONS}} \postmidterm{Post-M2}
% ============================================================

\subsection*{4.1 Joint Distributions}
\begin{itemize}[leftmargin=*]
\item \concept{Joint PMF (Discrete):} $p(x,y) = P(X=x, Y=y)$
\item \concept{Joint PDF (Continuous):} $f(x,y) \geq 0$
\item \concept{Normalization:} \formula{\int\int f(x,y)dxdy = 1}
\item \concept{Joint CDF:} \formula{F(x,y) = P(X \leq x, Y \leq y)}
\item \concept{When to Use:} Two or more random variables together
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Verify normalization (integral = 1)
  \item Find constant $c$ if needed
  \item Calculate probabilities over regions
  \end{enumerate}
\item \concept{Example:} $f(x,y) = c(x^2 + xy)$ on $[0,1]^2$, find $c = 12/7$
\item \warning{Check bounds carefully for integration!}
\end{itemize}

\subsection*{4.2 Marginal and Conditional Distributions}
\begin{itemize}[leftmargin=*]
\item \concept{Marginal PDF:} \formula{f_X(x) = \int_{-\infty}^{\infty} f(x,y)dy}
\item \concept{Marginal PMF:} $p_X(x) = \sum_y p(x,y)$
\item \concept{Conditional PDF:} \formula{f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)}}
\item \concept{Properties:} Conditional is a valid PDF/PMF
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Find marginal by integrating/summing out other variable
  \item For conditional, divide joint by marginal
  \item Verify it integrates to 1
  \end{enumerate}
\item \concept{Example:} Uniform on triangle, find conditional
\item \note{Bounds change for conditional distributions}
\end{itemize}

\subsection*{4.3 Independence of Random Variables}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} $X,Y$ independent iff $f(x,y) = f_X(x) \cdot f_Y(y)$
\item \concept{Test for Independence:}
  \begin{enumerate}
  \item Find joint distribution
  \item Find both marginals
  \item Check if product equals joint for ALL $(x,y)$
  \end{enumerate}
\item \concept{Consequences of Independence:}
  \begin{itemize}
  \item $E[XY] = E[X]E[Y]$
  \item $\text{Cov}(X,Y) = 0$ (but not vice versa!)
  \item $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$
  \end{itemize}
\item \warning{Zero covariance $\neq$ independence (except for normal)}
\item \note{For normal: independent $\Leftrightarrow \rho = 0$}
\end{itemize}

\subsection*{4.4 Covariance and Correlation}
\begin{itemize}[leftmargin=*]
\item \concept{Covariance:} \formula{\text{Cov}(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] = E[XY] - E[X]E[Y]}
\item \concept{Correlation:} \formula{\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}, \quad -1 \leq \rho \leq 1}
\item \concept{Properties:}
  \begin{itemize}
  \item $\text{Cov}(X,X) = \text{Var}(X)$
  \item $\text{Cov}(aX+b, cY+d) = ac \cdot \text{Cov}(X,Y)$
  \item $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)$
  \end{itemize}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Find $E[X]$, $E[Y]$, $E[XY]$
  \item Apply covariance formula
  \item For correlation, also find $\sigma_X$, $\sigma_Y$
  \end{enumerate}
\item \concept{Example:} HW4: Joint PDF, find $\rho$
\item \finance{Portfolio variance uses covariance matrix}
\end{itemize}

\subsection*{4.5 Bivariate Normal (a.k.a. Gaussian Vector, MVN, Jointly Normal)} \postmidterm{Critical!}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} \textbf{Gaussian vector} = \textbf{MVN} = \textbf{Multivariate Normal} = ``Jointly Normal''
\item \warning{``Independent components'' = $\rho = 0$ = \textbf{independence} (for MVN ONLY!)}
\item \concept{Definition:} $(X,Y)$ jointly normal with parameters $\mu_X, \mu_Y, \sigma_X^2, \sigma_Y^2, \rho$
\item \concept{Properties:}
  \begin{itemize}
  \item Linear combinations are normal
  \item Marginals are normal: $X \sim N(\mu_X, \sigma_X^2)$
  \item Independence $\Leftrightarrow \rho = 0$ (unique to normal!)
  \item Conditional is normal: $X|Y=y \sim N(\mu_{X|Y}, \sigma_{X|Y}^2)$
  \end{itemize}
\item \concept{Conditional Mean:} \formula{\mu_{X|Y} = \mu_X + \rho\frac{\sigma_X}{\sigma_Y}(y - \mu_Y)}
\item \concept{Conditional Variance:} \formula{\sigma_{X|Y}^2 = \sigma_X^2(1-\rho^2)}
\item \concept{Example:} HW5 Problem 1: Find $P(X+Y > 0)$
\item \note{$aX + bY$ is normal with specific mean/variance}
\end{itemize}

\subsection*{4.6 Transformations (a.k.a. Jacobian Method, CDF Method)} \postmidterm{Complex!}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``change of variables'', ``find distribution of $Y=g(X)$'', ``Jacobian''
\item \concept{Single Variable:} $Y = g(X)$
  \begin{itemize}
  \item CDF Method: Find $F_Y(y) = P(g(X) \leq y)$
  \item PDF Method: $f_Y(y) = f_X(g^{-1}(y))|dg^{-1}/dy|$
  \end{itemize}
\item \concept{Jacobian Method:} $(U,V) = g(X,Y)$
  \formula{f_{UV}(u,v) = f_{XY}(x(u,v), y(u,v)) \cdot |J|}
  where \formula{J = \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix}}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Define transformation
  \item Find inverse transformation
  \item Compute Jacobian determinant
  \item Apply formula with absolute value
  \item Check new bounds
  \end{enumerate}
\item \warning{Don't forget absolute value of Jacobian!}
\item \concept{Example:} Polar coordinates: $X = R\cos\Theta$, $Y = R\sin\Theta$
\end{itemize}

\subsection*{4.7 Order Statistics (a.k.a. Max/Min of i.i.d., $X_{(k)}$)}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``maximum'', ``minimum'', ``$k$-th smallest'', ``range''
\item \concept{Definition:} $X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$
\item \concept{Maximum:} \formula{F_{X_{(n)}}(x) = [F(x)]^n}
\item \concept{Minimum:} \formula{F_{X_{(1)}}(x) = 1 - [1-F(x)]^n}
\item \concept{PDF of $k$-th order statistic:}
  \formula{f_{X_{(k)}}(x) = \frac{n!}{(k-1)!(n-k)!}[F(x)]^{k-1}[1-F(x)]^{n-k}f(x)}
\item \concept{Example:} Max of 3 uniform(0,1) variables
\item \note{Range = $X_{(n)} - X_{(1)}$}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{5. MOMENT GENERATING FUNCTIONS}} \postmidterm{Post-M2}
% ============================================================

\subsection*{5.1 Definition and Properties (Prof. uses $\psi(t)$ for MGF)}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} MGF = $M_X(t)$ = $\psi(t)$ (professor's notation)
\item \concept{Definition:} \formula{M_X(t) = \psi(t) = E[e^{tX}] = \begin{cases} \sum e^{tx}p(x) & \text{discrete} \\ \int e^{tx}f(x)dx & \text{continuous} \end{cases}}
\item \concept{Moments:} \formula{E[X^k] = M^{(k)}(0)} (k-th derivative at 0)
\item \concept{Properties:}
  \begin{itemize}
  \item Uniqueness: Same MGF $\Rightarrow$ same distribution
  \item Linear: $M_{aX+b}(t) = e^{bt}M_X(at)$
  \item Sum of independent: $M_{X+Y}(t) = M_X(t) \cdot M_Y(t)$
  \end{itemize}
\item \concept{Common MGFs:}
  \begin{itemize}
  \item Binomial: $(1-p+pe^t)^n$
  \item Poisson: $e^{\lambda(e^t-1)}$
  \item Normal: $e^{\mu t + \sigma^2 t^2/2}$
  \item Exponential: $\lambda/(\lambda-t)$, $t < \lambda$
  \end{itemize}
\item \concept{Example:} Find distribution of sum using MGFs
\item \note{Not all distributions have MGF (e.g., Cauchy)}
\end{itemize}

\subsection*{5.2 Using MGFs for Sums}
\begin{itemize}[leftmargin=*]
\item \concept{Method:} For independent $X_1, ..., X_n$:
  \begin{enumerate}
  \item Find individual MGFs: $M_{X_i}(t)$
  \item Multiply: $M_S(t) = \prod M_{X_i}(t)$
  \item Match with known MGF to identify distribution
  \end{enumerate}
\item \concept{Example Applications:}
  \begin{itemize}
  \item Sum of normals is normal
  \item Sum of Poissons is Poisson
  \item Sum of gammas (same $\lambda$) is gamma
  \end{itemize}
\item \concept{Example:} $X_i \sim \text{Exp}(\lambda)$ independent, then $\sum X_i \sim \text{Gamma}(n, \lambda)$
\item \note{Powerful for proving CLT}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{6. LIMIT THEOREMS}} \postmidterm{Critical for Final!}
% ============================================================

\subsection*{6.1 Central Limit Theorem (a.k.a. CLT, Normal Approximation)} \postmidterm{Most Important!}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} CLT, ``approximate'', ``large n'', ``as $n \to \infty$'', ``normal approximation''
\item \concept{TRIGGER WORDS:} ``i.i.d.'', ``sample mean'', ``total/sum of n games'', ``average of n''
\item \concept{Statement:} If $X_1, ..., X_n$ are iid with mean $\mu$, variance $\sigma^2$:
  \formula{\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} N(0,1) \text{ as } n \to \infty}
\item \concept{Equivalent:} \formula{\bar{X}_n \approx N\left(\mu, \frac{\sigma^2}{n}\right) \text{ for large } n}
\item \concept{When to Use:}
  \begin{itemize}
  \item Large sample size (typically $n \geq 30$)
  \item Sum or average of many random variables
  \item Approximating discrete by continuous
  \end{itemize}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Verify conditions (iid, finite variance)
  \item Identify $\mu = E[X_i]$, $\sigma^2 = \text{Var}(X_i)$
  \item Standardize: $Z = (\bar{X} - \mu)/(\sigma/\sqrt{n})$
  \item Use normal table
  \end{enumerate}
\item \concept{Example:} 400 games, win \$3 with $p=0.25$, find $P(\text{total} > 240)$
\item \warning{Apply continuity correction for discrete!}
\end{itemize}

\subsection*{6.2 Normal Approximations}
\begin{itemize}[leftmargin=*]
\item \concept{Binomial Approximation:} If $X \sim \text{Binomial}(n,p)$ with $np(1-p) > 10$:
  \formula{X \approx N(np, np(1-p))}
\item \concept{Poisson Approximation:} If $X \sim \text{Poisson}(\lambda)$ with $\lambda > 30$:
  \formula{X \approx N(\lambda, \lambda)}
\item \concept{Continuity Correction:} For discrete $X$:
  \begin{itemize}
  \item $P(X = k) \approx P(k-0.5 < Y < k+0.5)$
  \item $P(X \leq k) \approx P(Y < k+0.5)$
  \item $P(X < k) \approx P(Y < k-0.5)$
  \end{itemize}
\item \concept{Example:} Binomial(100, 0.3), find $P(X > 35)$
\item \note{Correction improves accuracy significantly}
\end{itemize}

\subsection*{6.3 Law of Large Numbers (LLN)}
\begin{itemize}[leftmargin=*]
\item \concept{Weak LLN:} $\bar{X}_n \xrightarrow{P} \mu$ (convergence in probability)
\item \concept{Strong LLN:} $\bar{X}_n \xrightarrow{a.s.} \mu$ (almost sure convergence)
\item \concept{Interpretation:} Sample mean converges to true mean
\item \concept{Conditions:}
  \begin{itemize}
  \item Weak: Finite mean, pairwise uncorrelated
  \item Strong: Finite mean (iid case)
  \end{itemize}
\item \concept{Example:} Casino games, long-run frequency
\item \note{Foundation for frequentist probability}
\end{itemize}

\subsection*{6.4 Confidence Intervals}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Interval estimate with specified confidence level
\item \concept{For Mean (known $\sigma$):} \formula{\bar{X} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}}
\item \concept{Common Values:}
  \begin{itemize}
  \item 90\% CI: $z_{0.05} = 1.645$
  \item 95\% CI: $z_{0.025} = 1.96$
  \item 99\% CI: $z_{0.005} = 2.576$
  \end{itemize}
\item \concept{Interpretation:} 95\% of such intervals contain true parameter
\item \concept{Width:} $2 \cdot z_{\alpha/2} \cdot \sigma/\sqrt{n}$
\item \warning{NOT "95\% chance parameter is in interval"}
\item \note{Larger $n$ $\Rightarrow$ narrower interval}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{7. SPECIAL TOPICS \& APPLICATIONS}} \postmidterm{Post-M2}
% ============================================================

\subsection*{7.1 Conditional Expectation (a.k.a. $E[X|Y]$, Total Expectation)} \postmidterm{Conceptual!}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``$E[X|Y]$'', ``average given'', ``expected value given''
\item \concept{TRIGGER WORDS:} ``$E[X|Y=y]$'', ``break down by cases'', ``tower property''
\item \concept{Definition:}
  \begin{itemize}
  \item Discrete: $E[X|Y=y] = \sum x \cdot P(X=x|Y=y)$
  \item Continuous: $E[X|Y=y] = \int x \cdot f_{X|Y}(x|y)dx$
  \end{itemize}
\item \concept{Law of Total Expectation:} \formula{E[X] = E[E[X|Y]]}
\item \concept{Properties:}
  \begin{itemize}
  \item Linearity: $E[aX+bZ|Y] = aE[X|Y] + bE[Z|Y]$
  \item Taking out known: $E[h(Y)X|Y] = h(Y)E[X|Y]$
  \item Independence: $E[X|Y] = E[X]$ if independent
  \end{itemize}
\item \concept{Law of Total Variance:} \formula{\text{Var}(X) = E[\text{Var}(X|Y)] + \text{Var}(E[X|Y])}
\item \concept{Example:} Breaking sticks problem
\item \note{$E[X|Y]$ is a function of $Y$, not a number!}
\end{itemize}

\subsection*{7.2 Bayesian Statistics (a.k.a. Prior/Posterior, Conjugate Priors)} \postmidterm{Professor's Favorite!}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} ``prior'', ``posterior'', ``update belief'', ``given evidence'', ``conjugate''
\item \concept{TRIGGER WORDS:} ``defective rate'', ``unknown parameter'', ``given data'', ``Monty Hall''
\item \concept{Bayesian Framework:}
  \formula{\pi(\theta|x) = \frac{L(x|\theta)\pi(\theta)}{\int L(x|\theta)\pi(\theta)d\theta}}
  where Prior $\times$ Likelihood $\rightarrow$ Posterior
\item \concept{Conjugate Priors:}
  \begin{itemize}
  \item Beta-Binomial: $\text{Beta}(\alpha,\beta) \rightarrow \text{Beta}(\alpha+x, \beta+n-x)$
  \item Gamma-Poisson: $\text{Gamma}(\alpha,\beta) \rightarrow \text{Gamma}(\alpha+\sum x_i, \beta+n)$
  \item Normal-Normal: With known variance
  \end{itemize}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Specify prior $\pi(\theta)$
  \item Write likelihood $L(x|\theta)$
  \item Compute posterior (use conjugacy if possible)
  \item Normalize if needed
  \end{enumerate}
\item \concept{Example:} HW6 Monty Hall Bayesian analysis
\item \finance{Used in risk assessment, portfolio optimization}
\end{itemize}

\subsection*{7.3 Lognormal Distribution (a.k.a. $\ln X \sim N(\mu,\sigma^2)$)} \postmidterm{Finance Applications!}
\begin{itemize}[leftmargin=*]
\item \concept{SYNONYMS:} Lognormal, ``$\log X$ is normal'', ``$e^X$ where $X \sim N$'', ``stock price model''
\item \concept{TRIGGER WORDS:} ``stock price'', ``$S = S_0 e^Z$'', ``log returns'', ``always positive''
\item \concept{Definition:} $Y = e^X$ where $X \sim N(\mu, \sigma^2)$
\item \concept{Properties:}
  \begin{itemize}
  \item Always positive (good for prices)
  \item Right-skewed
  \item Mean: $E[Y] = e^{\mu + \sigma^2/2}$
  \item Variance: $\text{Var}(Y) = e^{2\mu+\sigma^2}(e^{\sigma^2}-1)$
  \item Median: $e^\mu$
  \end{itemize}
\item \concept{Stock Price Model:} $S_t = S_0 \exp(X_t)$ where $X_t \sim N(\mu t, \sigma^2 t)$
\item \concept{Example:} HW5 Problem 2, Practice Final stock problems
\item \finance{Black-Scholes model foundation}
\item \note{Log returns are normal, prices are lognormal}
\end{itemize}

\subsection*{7.4 Additional Important Concepts}
\begin{itemize}[leftmargin=*]
\item \concept{Indicator Random Variables:}
  \begin{itemize}
  \item $I_A = 1$ if $A$ occurs, 0 otherwise
  \item $E[I_A] = P(A)$
  \item Useful for counting: $\sum I_{A_i}$
  \end{itemize}
\item \concept{Jensen's Inequality:} For convex $g$: $E[g(X)] \geq g(E[X])$
\item \concept{Chebyshev's Inequality:} $P(|X-\mu| \geq k\sigma) \leq 1/k^2$
\item \concept{Probability Integral Transform:} $F(X) \sim \text{Uniform}(0,1)$
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{8. PRACTICE PROBLEM COMPENDIUM}}
% ============================================================

\subsection*{8.1 Bayesian Problems} \postmidterm{High Frequency!}
\textbf{Problem [HW6-1]: Monty Hall (Sober vs Dizzy)}\\
\textit{Contestant picks door A. Monty opens door B showing goat.}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Problem Type:} Bayesian update with different likelihoods
\item \textbf{Required Concepts:} Bayes' theorem, conditional probability
\item \textbf{Sober Monty:}
  \begin{itemize}[itemsep=0pt]
  \item Prior: $P(H_A) = P(H_B) = P(H_C) = 1/3$
  \item Likelihood: $P(\text{open B}|H_A) = 1/2$, $P(\text{open B}|H_B) = 0$, $P(\text{open B}|H_C) = 1$
  \item Posterior: $P(H_A|\text{data}) = 1/3$, $P(H_C|\text{data}) = 2/3$
  \item \textbf{Strategy: Switch! (doubles probability)}
  \end{itemize}
\item \textbf{Dizzy Monty:}
  \begin{itemize}[itemsep=0pt]
  \item Likelihood: $P(\text{open B}|H_A) = 1/2$, $P(\text{open B}|H_B) = 1/2$, $P(\text{open B}|H_C) = 1/2$
  \item Posterior: All equal at $1/3$
  \item \textbf{Strategy: Doesn't matter!}
  \end{itemize}
\item \textbf{Key Insight:} Knowledge affects likelihood function
\end{enumerate}

\subsection*{8.2 CLT Applications} \postmidterm{Guaranteed on Final!}
\textbf{Problem [Practice Final-1]: Coin Game with 400 Plays}\\
\textit{Win \$3 if HH, lose \$1 if TT, else \$0. Play 400 times.}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Problem Type:} CLT with discrete outcomes
\item \textbf{Step 1:} Find distribution of single game
  \begin{itemize}[itemsep=0pt]
  \item $P(X = 3) = 1/4$ (HH)
  \item $P(X = -1) = 1/4$ (TT)
  \item $P(X = 0) = 1/2$ (HT or TH)
  \end{itemize}
\item \textbf{Step 2:} Calculate $\mu$ and $\sigma^2$
  \begin{itemize}[itemsep=0pt]
  \item $E[X] = 3(1/4) + (-1)(1/4) + 0(1/2) = 0.5$
  \item $E[X^2] = 9(1/4) + 1(1/4) + 0 = 2.5$
  \item $\text{Var}(X) = 2.5 - 0.25 = 2.25$, so $\sigma = 1.5$
  \end{itemize}
\item \textbf{Step 3:} Apply CLT for $n = 400$
  \begin{itemize}[itemsep=0pt]
  \item Total: $S_{400} \approx N(400 \cdot 0.5, 400 \cdot 2.25) = N(200, 900)$
  \item $P(S_{400} \geq 240) = P(Z \geq \frac{240-200}{30}) = P(Z \geq 1.33) \approx 0.092$
  \end{itemize}
\item \textbf{Key Insight:} Use continuity correction: $P(S \geq 240) \approx P(S > 239.5)$
\end{enumerate}

\subsection*{8.3 Bivariate Normal} \postmidterm{Complex but Common!}
\textbf{Problem [HW5-1]: Joint Normal with Correlation}\\
\textit{$X \sim N(1, 2)$, $Y \sim N(-2, 3)$, $\rho = -2/3$. Find $P(X+Y > 0)$}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Problem Type:} Linear combination of bivariate normal
\item \textbf{Key Property:} $X + Y$ is normal
\item \textbf{Parameters of $Z = X + Y$:}
  \begin{itemize}[itemsep=0pt]
  \item $\mu_Z = \mu_X + \mu_Y = 1 + (-2) = -1$
  \item $\sigma_Z^2 = \sigma_X^2 + \sigma_Y^2 + 2\rho\sigma_X\sigma_Y = 2 + 3 + 2(-2/3)\sqrt{6} = 5 - \frac{4\sqrt{6}}{3}$
  \end{itemize}
\item \textbf{Standardize and compute:}
  \begin{itemize}[itemsep=0pt]
  \item $P(Z > 0) = P\left(\frac{Z+1}{\sigma_Z} > \frac{1}{\sigma_Z}\right) = 1 - \Phi(0.759) \approx 0.224$
  \end{itemize}
\item \textbf{Key Insight:} Always check if linear combination, use properties of bivariate normal
\end{enumerate}

\subsection*{8.4 Joint Distributions}
\textbf{Problem [HW4-1]: Joint PDF Analysis}\\
\textit{$f(x,y) = c(x^2 + xy)$ on $[0,1] \times [0,1]$}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Find constant $c$:}
  \begin{itemize}[itemsep=0pt]
  \item $\int_0^1 \int_0^1 (x^2 + xy) dx dy = \int_0^1 [\frac{x^3}{3} + \frac{x^2y}{2}]_0^1 dy = \int_0^1 (\frac{1}{3} + \frac{y}{2}) dy = \frac{7}{12}$
  \item Therefore $c = \frac{12}{7}$
  \end{itemize}
\item \textbf{Marginal of $X$:}
  \begin{itemize}[itemsep=0pt]
  \item $f_X(x) = \int_0^1 \frac{12}{7}(x^2 + xy) dy = \frac{12}{7}x^2 + \frac{6x}{7}$
  \end{itemize}
\item \textbf{Check independence:}
  \begin{itemize}[itemsep=0pt]
  \item Need $f(x,y) = f_X(x) \cdot f_Y(y)$ for all $(x,y)$
  \item Since $f(x,y)$ has $xy$ term, NOT independent
  \end{itemize}
\item \textbf{Key Insight:} Cross-product terms indicate dependence
\end{enumerate}

\subsection*{8.5 Lognormal Distribution} \finance{Finance Focus!}
\textbf{Problem [Practice Final-4]: Stock Price Model}\\
\textit{$S = S_0 e^Z$ where $Z \sim N((r-\sigma^2/2), \sigma^2)$, $S_0=100$, $r=0.05$, $\sigma=0.2$}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Problem Type:} Lognormal application
\item \textbf{Part (a):} Find $E[e^{-r}S] = E[S_0 e^{Z-r}]$
  \begin{itemize}[itemsep=0pt]
  \item $Z - r \sim N(-\sigma^2/2, \sigma^2)$
  \item $E[e^{Z-r}] = \exp(-\sigma^2/2 + \sigma^2/2) = 1$
  \item Therefore $E[e^{-r}S] = S_0 = 100$
  \end{itemize}
\item \textbf{Part (b):} Find $P(S > 100)$
  \begin{itemize}[itemsep=0pt]
  \item $P(S > 100) = P(e^Z > 1) = P(Z > 0)$
  \item $Z \sim N(-0.02, 0.04)$
  \item $P(Z > 0) = P\left(\frac{Z+0.02}{0.2} > 0.1\right) = 1 - \Phi(0.1) \approx 0.46$
  \end{itemize}
\item \textbf{Key Insight:} Stock prices lognormal $\Rightarrow$ log returns normal
\end{enumerate}

\subsection*{8.6 Exponential & Memoryless}
\textbf{Problem [Practice Final-3]: Average of Exponentials}\\
\textit{$X_1, ..., X_{100}$ iid Exp(1/3). Find $P(\bar{X}/(\bar{X}+3) < 0.5)$}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Problem Type:} CLT for exponentials
\item \textbf{Setup:} $E[X_i] = 3$, $\text{Var}(X_i) = 9$
\item \textbf{Apply CLT:} $\bar{X} \approx N(3, 9/100) = N(3, 0.09)$
\item \textbf{Transform inequality:}
  \begin{itemize}[itemsep=0pt]
  \item $\frac{\bar{X}}{\bar{X}+3} < 0.5 \Rightarrow \bar{X} < 0.5(\bar{X}+3) \Rightarrow \bar{X} < 3$
  \end{itemize}
\item \textbf{Calculate:} $P(\bar{X} < 3) = 0.5$ (by symmetry of normal)
\item \textbf{Key Insight:} Transform inequality first, then apply CLT
\end{enumerate}

\subsection*{8.7 Order Statistics}
\textbf{Problem:} Max and Min of Uniform(0,1)\\
\textit{$X_1, ..., X_n$ iid Uniform(0,1). Find distribution of max and min.}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Maximum $X_{(n)}$:}
  \begin{itemize}[itemsep=0pt]
  \item $F_{max}(x) = P(\text{all} \leq x) = x^n$
  \item $f_{max}(x) = nx^{n-1}$ for $0 < x < 1$
  \item $E[X_{(n)}] = \frac{n}{n+1}$
  \end{itemize}
\item \textbf{Minimum $X_{(1)}$:}
  \begin{itemize}[itemsep=0pt]
  \item $F_{min}(x) = 1 - P(\text{all} > x) = 1 - (1-x)^n$
  \item $f_{min}(x) = n(1-x)^{n-1}$ for $0 < x < 1$
  \item $E[X_{(1)}] = \frac{1}{n+1}$
  \end{itemize}
\item \textbf{Key Insight:} Use complement for min, direct for max
\end{enumerate}

\subsection*{8.8 Conjugate Priors} \postmidterm{Bayesian Favorite!}
\textbf{Problem [Practice Final-5]: Beta-Binomial Update}\\
\textit{Prior: $\theta \in \{1/2, 3/4\}$ equally likely. Data: 0 defects in 10 items.}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Problem Type:} Discrete prior Bayesian update
\item \textbf{Likelihoods:}
  \begin{itemize}[itemsep=0pt]
  \item $P(\text{0 defects}|\theta=1/2) = (1/2)^{10} = 1/1024$
  \item $P(\text{0 defects}|\theta=3/4) = (1/4)^{10} = 1/1048576$
  \end{itemize}
\item \textbf{Posterior:}
  \begin{itemize}[itemsep=0pt]
  \item $P(\theta=1/2|\text{data}) \propto (1/2) \cdot 1/1024 = 1/2048$
  \item $P(\theta=3/4|\text{data}) \propto (1/2) \cdot 1/1048576 \approx 0$
  \item After normalization: $P(\theta=1/2|\text{data}) \approx 0.999$
  \end{itemize}
\item \textbf{Key Insight:} Extreme data strongly favors lower defect rate
\end{enumerate}

\subsection*{8.9 Conditional Expectation}
\textbf{Problem:} Breaking Sticks\\
\textit{Break at $X \sim U(0,\ell)$, then break smaller piece at $Y|X \sim U(0,X)$}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Joint density:} $f(x,y) = \frac{1}{\ell} \cdot \frac{1}{x} = \frac{1}{\ell x}$ for $0 < y < x < \ell$
\item \textbf{Marginal of $Y$:} $f_Y(y) = \int_y^\ell \frac{1}{\ell x} dx = \frac{1}{\ell}\ln(\ell/y)$
\item \textbf{Conditional expectation:} $E[Y|X] = X/2$
\item \textbf{Total expectation:} $E[Y] = E[E[Y|X]] = E[X/2] = \ell/4$
\item \textbf{Total variance:} Use $\text{Var}(Y) = E[\text{Var}(Y|X)] + \text{Var}(E[Y|X])$
\item \textbf{Key Insight:} Hierarchical structure leads to law of total expectation
\end{enumerate}

\subsection*{8.10 Hypothesis Testing \& Confidence Intervals}
\textbf{Problem:} Test Average with CLT\\
\textit{Sample mean $\bar{X} = 52$ from $n=100$, known $\sigma=10$. Test $H_0: \mu = 50$}

\textbf{Solution:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Test statistic:} $Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} = \frac{52 - 50}{10/10} = 2$
\item \textbf{P-value:} $P(|Z| > 2) = 2(1 - \Phi(2)) = 0.0455$
\item \textbf{95\% CI:} $\bar{X} \pm 1.96 \cdot \sigma/\sqrt{n} = 52 \pm 1.96 = [50.04, 53.96]$
\item \textbf{Decision:} Reject $H_0$ at 5\% level (barely)
\item \textbf{Key Insight:} CI excludes 50, consistent with rejection
\end{enumerate}

% ============================================================
\section*{\textcolor{sectioncolor}{9. MULTI-STEP PROBLEM TEMPLATES}} \postmidterm{Critical!}
% ============================================================

\subsection*{Template A: Gaussian Vector Problems}
\textbf{When you see:} ``Gaussian vector'', ``independent components'', ``MVN''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Recognize:} ``Gaussian'' = Normal!
\item \textbf{``Independent components''} means $\rho = 0$ and for MVN: \textbf{independent!}
\item \textbf{Find Cov:} Set $\text{Cov}(Y_1, Y_2) = 0$ to find parameters
\item \textbf{Joint density:} Product of marginals (since independent)
\end{enumerate}

\textbf{Key Formula:} For $Y_1 = aX_1 + X_2$, $Y_2 = X_1 + bX_2$ (iid $N(0,1)$):\\
$\text{Cov}(Y_1,Y_2) = a\text{Var}(X_1) + b\text{Var}(X_2) = a + b$\\
Independence requires: $b = -a$

\subsection*{Template B: CLT Game/Coin Problems}
\textbf{When you see:} ``400 games'', ``total winnings'', ``approximate''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Define:} $X_i$ = single trial outcome
\item \textbf{PMF:} List outcomes and probabilities
\item \textbf{Compute:} $E[X_i] = \sum x \cdot P(X=x)$
\item \textbf{Compute:} $\text{Var}(X_i) = E[X^2] - (E[X])^2$
\item \textbf{CLT:} $S_n = \sum X_i \approx N(n\mu, n\sigma^2)$
\item \textbf{Standardize:} $Z = \frac{S_n - n\mu}{\sigma\sqrt{n}}$
\end{enumerate}

\textbf{Example:} Win \$3 if HH ($p=1/4$), lose \$1 if TT ($p=1/4$), else \$0 ($p=1/2$)\\
$E[X] = 3(1/4) - 1(1/4) + 0(1/2) = 1/2$\\
$E[X^2] = 9(1/4) + 1(1/4) = 10/4$, $\text{Var}(X) = 10/4 - 1/4 = 9/4$

\subsection*{Template C: Exponential + CLT}
\textbf{When you see:} ``i.i.d. Exp'', ``mean $\theta$'', ``average''

\warning{If ``mean $\theta = 3$'' then $\lambda = 1/3$ NOT 3!}

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Parameters:} $E[X_i] = 1/\lambda$, $\text{Var}(X_i) = 1/\lambda^2$
\item \textbf{For $\bar{X}$:} $E[\bar{X}] = 1/\lambda$, $\text{Var}(\bar{X}) = 1/(n\lambda^2)$
\item \textbf{CLT:} $\bar{X} \approx N(1/\lambda, 1/(n\lambda^2))$
\item \textbf{Transform inequality first:} e.g., $\bar{X}/(\bar{X}+3) < 0.5 \Rightarrow \bar{X} < 3$
\end{enumerate}

\subsection*{Template D: Lognormal Stock Price}
\textbf{When you see:} ``$S = S_0 e^Z$'', ``$Z \sim N(\mu,\sigma^2)$''

\textbf{Key Formulas:}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item $E[e^Z] = e^{\mu + \sigma^2/2}$ when $Z \sim N(\mu,\sigma^2)$
\item $E[S] = S_0 e^{\mu + \sigma^2/2}$
\item $P(S > K) = P(Z > \ln(K/S_0)) = 1 - \Phi\left(\frac{\ln(K/S_0) - \mu}{\sigma}\right)$
\end{itemize}

\textbf{For $E[e^{-r}S]$ with $Z \sim N(r-\sigma^2/2, \sigma^2)$:}\\
$E[e^{-r}S] = e^{-r}S_0 E[e^Z] = e^{-r}S_0 e^{(r-\sigma^2/2) + \sigma^2/2} = S_0$

\subsection*{Template E: Bayesian Discrete Prior}
\textbf{When you see:} ``prior'', ``posterior'', ``defective rate''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{List hypotheses:} $\theta_1, \theta_2, ...$
\item \textbf{Priors:} $P(\theta_i)$
\item \textbf{Likelihoods:} $P(\text{data}|\theta_i)$
\item \textbf{Bayes:} $P(\theta_i|\text{data}) = \frac{P(\text{data}|\theta_i)P(\theta_i)}{\sum_j P(\text{data}|\theta_j)P(\theta_j)}$
\item \textbf{Normalize:} Make sure posteriors sum to 1
\end{enumerate}

\subsection*{Template F: Bivariate Normal Conditional}
\textbf{When you see:} ``bivariate normal'', ``$Y|X=x$'', ``conditional distribution''

\textbf{Formula:}
\formula{Y|X=x \sim N\left(\mu_Y + \rho\frac{\sigma_Y}{\sigma_X}(x-\mu_X), (1-\rho^2)\sigma_Y^2\right)}

\textbf{Special Case:} If $\rho = 0$, then $Y|X=x \sim N(\mu_Y, \sigma_Y^2)$ (unchanged!)

\subsection*{Template G: Linear Combination Independence}
\textbf{When you see:} ``$Y_1 = aX_1 + X_2$'', ``$Y_2 = X_1 + bX_2$'', ``independent components''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Setup:} $X_1, X_2$ i.i.d. $N(0,1)$
\item \textbf{Key insight:} For linear combinations of Gaussians, independence $\Leftrightarrow$ zero covariance
\item \textbf{Calculate:} $\text{Cov}(Y_1, Y_2) = a\cdot 1 + b\cdot 1 = a + b$
\item \textbf{Solve:} $a + b = 0 \Rightarrow b = -a$
\item \textbf{Joint density:} Product of marginals (since independent)
\end{enumerate}

\subsection*{Template H: Predictive Distributions (Bayesian)}
\textbf{When you see:} ``predict next outcome'', ``predictive probability'', ``posterior predictive''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Prior predictive:} $P(X_{n+1}=x) = \sum_\theta P(X=x|\theta)P(\theta)$
\item \textbf{Posterior predictive:} $P(X_{n+1}=x|\text{data}) = \sum_\theta P(X=x|\theta)P(\theta|\text{data})$
\item \textbf{Use:} Posterior from Bayesian update in Step 2
\end{enumerate}

\textbf{Example:} Dice problem: Prior $P(\theta)$ for die type, observe data, predict next roll.

\subsection*{Template I: Product of Lognormals}
\textbf{When you see:} ``$XY$ where $X,Y$ are lognormal'', ``product of independent''

\textbf{Key Insight:} $\ln(XY) = \ln X + \ln Y$

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item If $\ln X \sim N(\mu_1, \sigma_1^2)$ and $\ln Y \sim N(\mu_2, \sigma_2^2)$ independent
\item Then $\ln(XY) \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$
\item So $XY$ is lognormal with parameters $(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2)$
\item $E[XY] = E[X]E[Y]$ (independence) $= e^{\mu_1+\sigma_1^2/2} \cdot e^{\mu_2+\sigma_2^2/2}$
\end{enumerate}

\subsection*{Template J: Monty Hall Variants}
\textbf{When you see:} ``Monty Hall'', ``contestant picks'', ``host opens''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Define hypotheses:} $H_A$, $H_B$, $H_C$ = car behind door A, B, C
\item \textbf{Priors:} Usually uniform $P(H_i) = 1/3$
\item \textbf{Key:} Likelihoods depend on host behavior!
  \begin{itemize}[itemsep=0pt]
  \item \textbf{Sober:} Knows car location, opens non-car door
  \item \textbf{Dizzy:} Opens random door (50-50)
  \end{itemize}
\item \textbf{Sober Monty:} Switch doubles probability (2/3 vs 1/3)
\item \textbf{Dizzy Monty:} No advantage to switching
\end{enumerate}

\subsection*{Template K: Finding n for CLT Probability}
\textbf{When you see:} ``smallest n such that'', ``how many samples needed''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Setup:} Want $P(\bar{X} > c) > p$ or $P(\bar{X} < c) > p$
\item \textbf{CLT:} $\bar{X} \approx N(\mu, \sigma^2/n)$
\item \textbf{Standardize:} $P\left(\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} > z\right) = $ target
\item \textbf{Solve for $n$:} Using $z$-table, find $z^*$ then solve $\frac{c-\mu}{\sigma/\sqrt{n}} = z^*$
\item \textbf{Result:} $n \geq \left(\frac{z^* \sigma}{c-\mu}\right)^2$
\end{enumerate}

\subsection*{Template L: Max/Min of i.i.d. Variables}
\textbf{When you see:} ``maximum of $n$'', ``minimum of $n$'', ``largest/smallest''

\textbf{Formulas:}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item $P(\max < a) = P(\text{all} < a) = [F(a)]^n$ (if i.i.d.)
\item $P(\max > a) = 1 - [F(a)]^n$
\item $P(\min < a) = 1 - [1-F(a)]^n$
\item $P(\min > a) = [1-F(a)]^n$
\end{itemize}

\textbf{For Uniform(0,1):} $E[X_{(n)}] = \frac{n}{n+1}$, $E[X_{(1)}] = \frac{1}{n+1}$

\subsection*{Template M: Conditioning on Event}
\textbf{When you see:} ``given that $X > a$'', ``conditional on event''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Conditional CDF:} $P(X \leq x | X > a) = \frac{P(a < X \leq x)}{P(X > a)}$ for $x > a$
\item \textbf{For Exponential:} Memoryless! $P(X > s+t | X > s) = P(X > t)$
\item \textbf{General:} Use $f_{X|A}(x) = f_X(x)/P(A)$ for $x \in A$
\end{enumerate}

\subsection*{Template N: Bivariate Normal from Conditions}
\textbf{When you see:} ``$E[Y|X=x] = ...$'', ``$\text{Var}(Y|X) = ...$'', ``find parameters''

\textbf{Key Formulas:}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item $E[Y|X=x] = \mu_Y + \rho\frac{\sigma_Y}{\sigma_X}(x - \mu_X)$
\item $\text{Var}(Y|X) = \sigma_Y^2(1-\rho^2)$ (constant!)
\end{itemize}

\textbf{Method:} Match coefficients to extract $\mu_Y$, $\rho\frac{\sigma_Y}{\sigma_X}$, and $\sigma_Y^2(1-\rho^2)$

\subsection*{Template O: Probability Involving Sample Average}
\textbf{When you see:} ``$P(\bar{X}/(\bar{X}+c) < p)$'', ``ratio with sample mean''

\textbf{Steps:}
\begin{enumerate}[itemsep=0pt]
\item \textbf{Transform:} Simplify inequality algebraically first!
\item \textbf{Example:} $\frac{\bar{X}}{\bar{X}+3} < 0.5 \Leftrightarrow \bar{X} < 3$
\item \textbf{Apply CLT:} $\bar{X} \approx N(\mu, \sigma^2/n)$
\item \textbf{Calculate:} Standard normal probability
\end{enumerate}

\subsection*{Template P: Sum of Independent Poissons}
\textbf{When you see:} ``sum of Poisson'', ``combined arrivals''

\textbf{Key Property:} If $X \sim \text{Poisson}(\lambda_1)$, $Y \sim \text{Poisson}(\lambda_2)$ independent:
\formula{X + Y \sim \text{Poisson}(\lambda_1 + \lambda_2)}

\textbf{Method:} Use MGF: $M_{X+Y}(t) = e^{\lambda_1(e^t-1)} \cdot e^{\lambda_2(e^t-1)} = e^{(\lambda_1+\lambda_2)(e^t-1)}$

% ============================================================
% APPENDICES
% ============================================================

\section*{\textcolor{sectioncolor}{APPENDIX A: COMPLETE FORMULA SHEET}}
\begin{multicols}{2}
\subsection*{Probability Formulas}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
\item $P(A^c) = 1 - P(A)$
\item $P(A|B) = P(A \cap B)/P(B)$
\item $P(A \cap B) = P(B)P(A|B) = P(A)P(B|A)$
\item $P(A) = \sum P(A|B_i)P(B_i)$ (Total Probability)
\item $P(H|E) = P(E|H)P(H)/P(E)$ (Bayes)
\item $C(n,k) = n!/(k!(n-k)!)$
\item $P(n,k) = n!/(n-k)!$
\end{itemize}

\subsection*{Expectation \& Variance}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item $E[X] = \sum x p(x)$ or $\int x f(x) dx$
\item $E[g(X)] = \sum g(x) p(x)$ or $\int g(x) f(x) dx$
\item $E[aX + b] = aE[X] + b$
\item $E[X + Y] = E[X] + E[Y]$
\item $\text{Var}(X) = E[X^2] - (E[X])^2$
\item $\text{Var}(aX + b) = a^2 \text{Var}(X)$
\item $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)$
\item $\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$
\item $\rho = \text{Cov}(X,Y)/(\sigma_X \sigma_Y)$
\end{itemize}

\subsection*{Conditional Expectation}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item $E[X|Y=y] = \sum x P(X=x|Y=y)$ or $\int x f(x|y) dx$
\item $E[X] = E[E[X|Y]]$ (Total Expectation)
\item $E[h(Y)X|Y] = h(Y)E[X|Y]$
\item $\text{Var}(X) = E[\text{Var}(X|Y)] + \text{Var}(E[X|Y])$
\end{itemize}

\subsection*{MGF \& Limit Theorems}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item $M_X(t) = E[e^{tX}]$
\item $E[X^k] = M^{(k)}(0)$
\item $M_{X+Y}(t) = M_X(t) M_Y(t)$ (if independent)
\item CLT: $\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} N(0,1)$
\item CI: $\bar{X} \pm z_{\alpha/2} \sigma/\sqrt{n}$
\end{itemize}
\end{multicols}

\section*{\textcolor{sectioncolor}{APPENDIX B: DISTRIBUTION CHEAT SHEET}}
{\small
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Distribution} & \textbf{PMF/PDF} & \textbf{Mean} & \textbf{Variance} & \textbf{MGF} & \textbf{Notes} \\
\hline
\multicolumn{6}{|c|}{\textbf{Discrete Distributions}} \\
\hline
Bernoulli($p$) & $p^x(1-p)^{1-x}$ & $p$ & $p(1-p)$ & $(1-p)+pe^t$ & $x \in \{0,1\}$ \\
Binomial($n,p$) & $\binom{n}{k}p^k(1-p)^{n-k}$ & $np$ & $np(1-p)$ & $(1-p+pe^t)^n$ & $k = 0,...,n$ \\
Poisson($\lambda$) & $e^{-\lambda}\lambda^k/k!$ & $\lambda$ & $\lambda$ & $e^{\lambda(e^t-1)}$ & $k = 0,1,2,...$ \\
Geometric($p$) & $p(1-p)^k$ & $(1-p)/p$ & $(1-p)/p^2$ & $p/(1-(1-p)e^t)$ & Memoryless \\
Neg. Binomial($r,p$) & $\binom{k+r-1}{k}p^r(1-p)^k$ & $r(1-p)/p$ & $r(1-p)/p^2$ & $(p/(1-(1-p)e^t))^r$ & -- \\
Hypergeometric & Complex & $n K/N$ & Complex & -- & No replacement \\
\hline
\multicolumn{6}{|c|}{\textbf{Continuous Distributions}} \\
\hline
Uniform($a,b$) & $1/(b-a)$ & $(a+b)/2$ & $(b-a)^2/12$ & $(e^{bt}-e^{at})/(t(b-a))$ & -- \\
Normal($\mu,\sigma^2$) & $(2\pi\sigma^2)^{-1/2}e^{-(x-\mu)^2/(2\sigma^2)}$ & $\mu$ & $\sigma^2$ & $e^{\mu t + \sigma^2 t^2/2}$ & -- \\
Exponential($\lambda$) & $\lambda e^{-\lambda x}$ & $1/\lambda$ & $1/\lambda^2$ & $\lambda/(\lambda-t)$ & Memoryless \\
Gamma($r,\lambda$) & $\lambda^r x^{r-1}e^{-\lambda x}/\Gamma(r)$ & $r/\lambda$ & $r/\lambda^2$ & $(\lambda/(\lambda-t))^r$ & -- \\
Beta($\alpha,\beta$) & $x^{\alpha-1}(1-x)^{\beta-1}/B(\alpha,\beta)$ & $\alpha/(\alpha+\beta)$ & Complex & -- & $x \in (0,1)$ \\
\hline
\end{tabular}
}

\section*{\textcolor{sectioncolor}{APPENDIX C: PROFESSOR'S NOTATION GUIDE}}
\begin{multicols}{2}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item Uses $\psi(t)$ for MGF (not $M(t)$)
\item Writes $\text{Var}(X)$ not $\sigma^2_X$
\item Uses $f(x)$ for both PMF and PDF
\item $g_1(x|y)$ for conditional PDF of $X|Y$
\item $\pi(\theta)$ for prior, $\pi(\theta|x)$ for posterior
\item $L(x|\theta)$ for likelihood
\item $H_i$ for hypotheses in Bayes problems
\item $\bar{X}$ or $\bar{X}_n$ for sample mean
\item $X_{(k)}$ for $k$-th order statistic
\item $I_A$ for indicator of event $A$
\item $\xrightarrow{d}$ for convergence in distribution
\item $\xrightarrow{P}$ for convergence in probability
\item $\Phi(z)$ for standard normal CDF
\item $z_\alpha$ for quantile where $P(Z > z_\alpha) = \alpha$
\item Finance: $S_t$ for stock price at time $t$
\end{itemize}
\end{multicols}

\section*{\textcolor{sectioncolor}{APPENDIX D: LAST-MINUTE REVIEW CHECKLIST}}

\subsection*{Time Management (90 minutes, 3 questions)}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item[$\square$] \textbf{0-5 min:} Read all problems, identify types using decision tree
\item[$\square$] \textbf{5-35 min:} Question 1 (aim for 30 min max)
\item[$\square$] \textbf{35-65 min:} Question 2 (aim for 30 min max)
\item[$\square$] \textbf{65-85 min:} Question 3 (aim for 20 min)
\item[$\square$] \textbf{85-90 min:} Review, check arithmetic
\end{itemize}

\subsection*{High-Yield Topics to Review (Post-M2 Focus)}
\begin{enumerate}[leftmargin=*,itemsep=0pt]
\item \postmidterm{Central Limit Theorem applications}
\item \postmidterm{Bivariate Normal problems}
\item \postmidterm{Bayesian updates (especially Monty Hall variants)}
\item \postmidterm{Conditional Expectation and Total Expectation}
\item \postmidterm{Lognormal/Finance applications}
\item Joint distributions (finding marginals, checking independence)
\item Covariance and correlation calculations
\item MGF for finding distributions of sums
\item Normal approximations with continuity correction
\item Confidence intervals using CLT
\end{enumerate}

\subsection*{What to Memorize vs Look Up}
\textbf{MEMORIZE:}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item ``Gaussian'' = Normal, ``Gaussian vector'' = MVN
\item Normal standardization: $Z = (X-\mu)/\sigma$
\item CLT: $(\bar{X}-\mu)/(\sigma/\sqrt{n}) \to N(0,1)$
\item Lognormal: $E[e^X] = e^{\mu + \sigma^2/2}$ for $X \sim N(\mu,\sigma^2)$
\item BVN Conditional: $\mu_{Y|X} = \mu_Y + \rho(\sigma_Y/\sigma_X)(x-\mu_X)$
\item For MVN ONLY: $\rho = 0 \Leftrightarrow$ independent
\end{itemize}

\textbf{LOOK UP:}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item Distribution tables (PMF/PDF formulas)
\item MGF formulas: $\psi(t)$ values
\item Jacobian details
\item Normal table ($\Phi$ values)
\end{itemize}

\subsection*{PARAMETER TRAP CHECKLIST}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item[$\square$] ``Mean $\theta=3$'' (Exp) $\Rightarrow$ $\lambda = 1/3$
\item[$\square$] ``Rate $\lambda=2$'' $\Rightarrow$ Mean $= 1/2$
\item[$\square$] Check: Is it Geom(failures) or Geom(trials)?
\item[$\square$] BVN: Is variance $\sigma^2$ or std dev $\sigma$?
\end{itemize}

\subsection*{Common Professor Patterns}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item Part (a): Basic setup/calculation
\item Part (b): Extension requiring part (a)
\item Part (c): Conceptual twist or limiting behavior
\item Finance context in at least one problem
\item One Bayesian problem guaranteed
\item One CLT/approximation problem guaranteed
\end{itemize}

\subsection*{Final Tips}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item \warning{Always check if variables are independent before using simplified formulas}
\item \warning{Apply continuity correction for discrete $\rightarrow$ continuous}
\item \warning{Verify bounds of integration match the region}
\item Start with problems you recognize immediately
\item Show all work - partial credit is generous
\item If stuck, write down relevant formulas and what you know
\item Check units/reasonableness of final answers
\end{itemize}

\end{multicols}
\end{document}
