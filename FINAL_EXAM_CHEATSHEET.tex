\documentclass[10pt,landscape,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.5in]{geometry}
\usepackage{multicol}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}

% Custom commands for consistent formatting
\newcommand{\concept}[1]{\textbf{#1}}
\newcommand{\formula}[1]{\boxed{#1}}
\newcommand{\note}[1]{\textit{Note: #1}}
\newcommand{\warning}[1]{\textcolor{red}{\textbf{âš  #1}}}
\newcommand{\finance}[1]{\textcolor{blue}{\textbf{ðŸ’° #1}}}
\newcommand{\postmidterm}[1]{\textcolor{orange}{\textbf{ðŸ”¥ #1}}}
\newcommand{\quickref}[2]{\textbf{#1:} #2}

% Define colors
\definecolor{sectioncolor}{RGB}{0,102,204}
\definecolor{formulacolor}{RGB}{0,153,0}

\begin{document}

\begin{center}
{\Huge \textbf{Probability Theory Final Exam Cheat Sheet}}\\
\vspace{0.2cm}
{\large December 16, 2025 | 7:10pm-8:40pm | 3 Questions, 1.5 Hours}\\
{\small \textit{Open Book Exam - Focus on Post-Midterm 2 Material}}
\end{center}

\begin{multicols}{3}

% ============================================================
\section*{\textcolor{sectioncolor}{0. QUICK REFERENCE GUIDE}}
% ============================================================

\subsection*{Decision Tree Summary}
\begin{enumerate}[leftmargin=*,itemsep=0pt]
\item \textbf{Probability?} â†’ Conditional/Bayes/Basic
\item \textbf{Random Variable?} â†’ Discrete/Continuous
\item \textbf{Multiple Variables?} â†’ Joint/Independence/Correlation
\item \textbf{Expectations?} â†’ Basic/Variance/Conditional/MGF
\item \textbf{Large Sample?} â†’ CLT/Normal Approx/LLN
\item \textbf{Bayesian?} â†’ Update/Conjugate/Predictive
\item \textbf{Finance?} â†’ Lognormal/Portfolio
\end{enumerate}

\subsection*{Top 20 Critical Formulas}
\begin{enumerate}[leftmargin=*,itemsep=0pt,topsep=0pt]
\item \formula{P(A|B) = \frac{P(A \cap B)}{P(B)}}
\item \formula{P(H|E) = \frac{P(E|H)P(H)}{P(E)}} (Bayes)
\item \formula{P(A) = \sum P(A|B_i)P(B_i)} (Total Prob)
\item \formula{E[X] = \sum x P(X=x)} (Discrete)
\item \formula{E[X] = \int x f(x)dx} (Continuous)
\item \formula{\text{Var}(X) = E[X^2] - (E[X])^2}
\item \formula{\text{Cov}(X,Y) = E[XY] - E[X]E[Y]}
\item \formula{\rho = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}}
\item \formula{P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}} (Binomial)
\item \formula{P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}} (Poisson)
\item \formula{f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}} (Normal)
\item \formula{Z = \frac{X-\mu}{\sigma}} (Standardization)
\item \formula{M(t) = E[e^{tX}]} (MGF)
\item \formula{E[X] = E[E[X|Y]]} (Total Expectation)
\item \formula{\text{Var}(X) = E[\text{Var}(X|Y)] + \text{Var}(E[X|Y])}
\item \formula{Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)} (CLT)
\item \formula{\text{CI}: \bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}}
\item \formula{\pi(\theta|x) \propto L(x|\theta)\pi(\theta)} (Bayes)
\item \formula{E[e^X] = e^{\mu + \sigma^2/2}} (Lognormal)
\item \formula{f_{UV}(u,v) = f_{XY}(x,y)|J|} (Jacobian)
\end{enumerate}

\subsection*{Common Mistakes Checklist}
\begin{itemize}[leftmargin=*,itemsep=0pt]
\item[$\square$] Forgot continuity correction for discreteâ†’normal
\item[$\square$] Confused $P(A|B)$ with $P(B|A)$
\item[$\square$] Didn't check independence before using formulas
\item[$\square$] Wrong integration limits for marginals
\item[$\square$] Forgot to normalize Bayesian posterior
\item[$\square$] Used Binomial instead of Hypergeometric
\item[$\square$] Forgot absolute value of Jacobian
\item[$\square$] Assumed correlation implies causation
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{1. FUNDAMENTAL CONCEPTS}}
% ============================================================

\subsection*{1.1 Probability Axioms}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} A probability measure satisfies:
  \begin{enumerate}
  \item Normalization: $P(S) = 1$
  \item Non-negativity: $P(A) \geq 0$
  \item Additivity: $P(A \cup B) = P(A) + P(B)$ if $A \cap B = \emptyset$
  \end{enumerate}
\item \concept{Key Formula:} \formula{P(A \cup B) = P(A) + P(B) - P(A \cap B)}
\item \concept{When to Use:} Basic probability calculations
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Identify sample space $S$
  \item Count favorable outcomes
  \item Apply formula
  \end{enumerate}
\item \concept{Example:} Two dice: $P(\text{sum}=7) = 6/36 = 1/6$
\item \concept{Common Pitfalls:} Forgetting the intersection term
\item \note{Equally likely: $P(A) = |A|/|S|$}
\end{itemize}

\subsection*{1.2 Conditional Probability}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Probability of $A$ given $B$ occurred
\item \concept{Key Formula:} \formula{P(A|B) = \frac{P(A \cap B)}{P(B)}, \text{ if } P(B) > 0}
\item \concept{When to Use:} "Given that", "if we know", "conditional on"
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Identify condition $B$ and target $A$
  \item Find $P(A \cap B)$ and $P(B)$
  \item Apply formula
  \end{enumerate}
\item \concept{Example:} Roll dice, sum odd. $P(\text{sum}<8|\text{odd}) = 2/3$
\item \concept{Common Pitfalls:} Confusing $P(A|B)$ with $P(B|A)$
\item \note{Multiplication Rule: $P(A \cap B) = P(B)P(A|B)$}
\end{itemize}

\subsection*{1.3 Bayes' Theorem}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Update probability given evidence
\item \concept{Key Formula:} \formula{P(H_i|E) = \frac{P(E|H_i)P(H_i)}{\sum_j P(E|H_j)P(H_j)}}
\item \concept{When to Use:} "Update", "posterior", "given evidence"
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item List hypotheses $H_i$ with priors $P(H_i)$
  \item Find likelihoods $P(E|H_i)$
  \item Apply Bayes' formula
  \item Normalize if needed
  \end{enumerate}
\item \concept{Example:} Monty Hall: Switch wins 2/3 of time
\item \concept{Common Pitfalls:} Wrong likelihood, forgetting to normalize
\item \note{Total Probability: $P(A) = \sum P(A|B_i)P(B_i)$}
\end{itemize}

\subsection*{1.4 Independence}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} $A$ and $B$ independent if $P(A \cap B) = P(A)P(B)$
\item \concept{Key Formula:} \formula{P(A|B) = P(A) \text{ iff independent}}
\item \concept{When to Use:} Testing if events affect each other
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Calculate $P(A)$, $P(B)$, $P(A \cap B)$
  \item Check if $P(A \cap B) = P(A) \cdot P(B)$
  \item State conclusion
  \end{enumerate}
\item \concept{Example:} Card draws with replacement are independent
\item \concept{Common Pitfalls:} Assuming independence without checking
\item \note{Pairwise $\neq$ Mutual independence}
\end{itemize}

\subsection*{1.5 Counting Methods}
\begin{itemize}[leftmargin=*]
\item \concept{Permutations:} Order matters
  \formula{P(n,k) = \frac{n!}{(n-k)!}}
\item \concept{Combinations:} Order doesn't matter
  \formula{C(n,k) = \binom{n}{k} = \frac{n!}{k!(n-k)!}}
\item \concept{Multinomial:} Multiple categories
  \formula{\frac{n!}{n_1!n_2!\cdots n_k!}}
\item \concept{When to Use:} "How many ways", "arrangements", "selections"
\item \concept{Example:} 6-card poker hands from 52 cards: $\binom{52}{6}$
\item \note{With replacement: $n^k$; Without: $P(n,k)$ or $C(n,k)$}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{2. DISCRETE RANDOM VARIABLES}}
% ============================================================

\subsection*{2.1 PMF and CDF}
\begin{itemize}[leftmargin=*]
\item \concept{PMF:} $p(x) = P(X = x)$, where $\sum p(x) = 1$
\item \concept{CDF:} $F(x) = P(X \leq x) = \sum_{k \leq x} p(k)$
\item \concept{Expectation:} \formula{E[X] = \sum x \cdot P(X=x)}
\item \concept{Variance:} \formula{\text{Var}(X) = E[X^2] - (E[X])^2}
\item \concept{Properties:} CDF is right-continuous, non-decreasing
\item \note{$P(a < X \leq b) = F(b) - F(a)$}
\end{itemize}

\subsection*{2.2 Binomial Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Number of successes in $n$ independent trials
\item \concept{PMF:} \formula{P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}}
\item \concept{Mean:} $E[X] = np$
\item \concept{Variance:} $\text{Var}(X) = np(1-p)$
\item \concept{MGF:} $M(t) = (1-p+pe^t)^n$
\item \concept{When to Use:} Fixed $n$, constant $p$, independent trials
\item \concept{Example:} Flip coin 10 times, $P(X=6)$ heads with $p=0.5$
\item \warning{Check conditions before using!}
\item \note{Normal approximation when $np(1-p) > 10$}
\end{itemize}

\subsection*{2.3 Poisson Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Count of rare events in fixed interval
\item \concept{PMF:} \formula{P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}}
\item \concept{Mean:} $E[X] = \lambda$
\item \concept{Variance:} $\text{Var}(X) = \lambda$
\item \concept{MGF:} $M(t) = e^{\lambda(e^t-1)}$
\item \concept{When to Use:} Rate $\lambda$ per unit time/space
\item \concept{Example:} Arrivals per hour, defects per batch
\item \concept{Property:} Sum of independent Poissons is Poisson
\item \note{Approximates Binomial when $n$ large, $p$ small, $np = \lambda$}
\end{itemize}

\subsection*{2.4 Geometric Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Number of failures before first success
\item \concept{PMF:} \formula{P(X=k) = p(1-p)^k, \quad k=0,1,2,...}
\item \concept{Mean:} $E[X] = (1-p)/p$
\item \concept{Variance:} $\text{Var}(X) = (1-p)/p^2$
\item \concept{Memoryless:} $P(X=m+n|X \geq m) = P(X=n)$
\item \concept{When to Use:} "First success", "waiting time"
\item \concept{Example:} Roll die until first 6 appears
\item \note{Alternative parameterization: trials until success}
\end{itemize}

\subsection*{2.5 Negative Binomial}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Failures before $r$-th success
\item \concept{PMF:} \formula{P(X=k) = \binom{k+r-1}{k}p^r(1-p)^k}
\item \concept{Mean:} $E[X] = r(1-p)/p$
\item \concept{Variance:} $\text{Var}(X) = r(1-p)/p^2$
\item \concept{When to Use:} "$r$-th success", extended geometric
\item \note{Geometric is special case with $r=1$}
\end{itemize}

\subsection*{2.6 Hypergeometric Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Sampling without replacement
\item \concept{PMF:} \formula{P(X=k) = \frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}}
\item \concept{Parameters:} $N$ total, $K$ success, $n$ sample, $k$ observed
\item \concept{Mean:} $E[X] = n \cdot K/N$
\item \concept{When to Use:} Finite population, no replacement
\item \concept{Example:} Draw 5 cards, probability of 3 aces
\item \warning{Different from Binomial (with replacement)}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{3. CONTINUOUS RANDOM VARIABLES}}
% ============================================================

\subsection*{3.1 PDF and CDF}
\begin{itemize}[leftmargin=*]
\item \concept{PDF:} $f(x) \geq 0$, $\int_{-\infty}^{\infty} f(x)dx = 1$
\item \concept{CDF:} \formula{F(x) = P(X \leq x) = \int_{-\infty}^x f(t)dt}
\item \concept{Probability:} \formula{P(a < X < b) = \int_a^b f(x)dx}
\item \concept{Expectation:} \formula{E[X] = \int_{-\infty}^{\infty} x f(x)dx}
\item \concept{Variance:} \formula{\text{Var}(X) = \int (x-\mu)^2 f(x)dx}
\item \concept{Relation:} $f(x) = F'(x)$ where derivative exists
\item \note{$P(X = a) = 0$ for continuous RV}
\end{itemize}

\subsection*{3.2 Uniform Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Equally likely over interval $[a,b]$
\item \concept{PDF:} \formula{f(x) = \frac{1}{b-a}, \quad a \leq x \leq b}
\item \concept{CDF:} $F(x) = \frac{x-a}{b-a}$ for $a \leq x \leq b$
\item \concept{Mean:} $E[X] = \frac{a+b}{2}$
\item \concept{Variance:} $\text{Var}(X) = \frac{(b-a)^2}{12}$
\item \concept{When to Use:} "Equally likely", "random point"
\item \concept{Example:} Random number between 0 and 1
\item \note{Probability proportional to interval length}
\end{itemize}

\subsection*{3.3 Normal Distribution} \postmidterm{High Priority!}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Bell curve, most important continuous distribution
\item \concept{PDF:} \formula{f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}}
\item \concept{Notation:} $X \sim N(\mu, \sigma^2)$
\item \concept{Standardization:} \formula{Z = \frac{X-\mu}{\sigma} \sim N(0,1)}
\item \concept{Properties:}
  \begin{itemize}
  \item Linear combination: $aX+b \sim N(a\mu+b, a^2\sigma^2)$
  \item Sum of normals: normal
  \item 68-95-99.7 rule for $\pm 1,2,3$ std dev
  \end{itemize}
\item \concept{MGF:} $M(t) = e^{\mu t + \sigma^2 t^2/2}$
\item \concept{Example:} Heights, measurement errors, CLT limit
\item \note{Use $\Phi(z)$ table for standard normal CDF}
\end{itemize}

\subsection*{3.4 Exponential Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Waiting time until event
\item \concept{PDF:} \formula{f(x) = \lambda e^{-\lambda x}, \quad x > 0}
\item \concept{CDF:} $F(x) = 1 - e^{-\lambda x}$
\item \concept{Mean:} $E[X] = 1/\lambda$
\item \concept{Variance:} $\text{Var}(X) = 1/\lambda^2$
\item \concept{Memoryless:} \formula{P(X > s+t | X > s) = P(X > t)}
\item \concept{MGF:} $M(t) = \frac{\lambda}{\lambda - t}$ for $t < \lambda$
\item \concept{When to Use:} Time between Poisson events
\item \concept{Example:} Service times, component lifetime
\item \note{Min of exponentials is exponential}
\end{itemize}

\subsection*{3.5 Gamma Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Sum of exponentials, generalization
\item \concept{PDF:} \formula{f(x) = \frac{\lambda^r}{\Gamma(r)}x^{r-1}e^{-\lambda x}, \quad x > 0}
\item \concept{Mean:} $E[X] = r/\lambda$
\item \concept{Variance:} $\text{Var}(X) = r/\lambda^2$
\item \concept{Special Cases:}
  \begin{itemize}
  \item $r=1$: Exponential($\lambda$)
  \item $r=n/2, \lambda=1/2$: Chi-square with $n$ df
  \end{itemize}
\item \concept{When to Use:} Time until $r$-th event
\item \note{$\Gamma(n) = (n-1)!$ for integer $n$}
\end{itemize}

\subsection*{3.6 Beta Distribution}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Models probabilities/proportions
\item \concept{PDF:} \formula{f(x) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}}
\item \concept{Support:} $0 < x < 1$
\item \concept{Mean:} $E[X] = \frac{\alpha}{\alpha+\beta}$
\item \concept{Variance:} $\text{Var}(X) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$
\item \concept{Special Cases:}
  \begin{itemize}
  \item $\alpha=\beta=1$: Uniform(0,1)
  \item Conjugate prior for Binomial
  \end{itemize}
\item \finance{Used in Bayesian statistics}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{4. MULTIVARIATE DISTRIBUTIONS}} \postmidterm{Post-M2}
% ============================================================

\subsection*{4.1 Joint Distributions}
\begin{itemize}[leftmargin=*]
\item \concept{Joint PMF (Discrete):} $p(x,y) = P(X=x, Y=y)$
\item \concept{Joint PDF (Continuous):} $f(x,y) \geq 0$
\item \concept{Normalization:} \formula{\int\int f(x,y)dxdy = 1}
\item \concept{Joint CDF:} \formula{F(x,y) = P(X \leq x, Y \leq y)}
\item \concept{When to Use:} Two or more random variables together
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Verify normalization (integral = 1)
  \item Find constant $c$ if needed
  \item Calculate probabilities over regions
  \end{enumerate}
\item \concept{Example:} $f(x,y) = c(x^2 + xy)$ on $[0,1]^2$, find $c = 12/7$
\item \warning{Check bounds carefully for integration!}
\end{itemize}

\subsection*{4.2 Marginal and Conditional Distributions}
\begin{itemize}[leftmargin=*]
\item \concept{Marginal PDF:} \formula{f_X(x) = \int_{-\infty}^{\infty} f(x,y)dy}
\item \concept{Marginal PMF:} $p_X(x) = \sum_y p(x,y)$
\item \concept{Conditional PDF:} \formula{f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)}}
\item \concept{Properties:} Conditional is a valid PDF/PMF
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Find marginal by integrating/summing out other variable
  \item For conditional, divide joint by marginal
  \item Verify it integrates to 1
  \end{enumerate}
\item \concept{Example:} Uniform on triangle, find conditional
\item \note{Bounds change for conditional distributions}
\end{itemize}

\subsection*{4.3 Independence of Random Variables}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} $X,Y$ independent iff $f(x,y) = f_X(x) \cdot f_Y(y)$
\item \concept{Test for Independence:}
  \begin{enumerate}
  \item Find joint distribution
  \item Find both marginals
  \item Check if product equals joint for ALL $(x,y)$
  \end{enumerate}
\item \concept{Consequences of Independence:}
  \begin{itemize}
  \item $E[XY] = E[X]E[Y]$
  \item $\text{Cov}(X,Y) = 0$ (but not vice versa!)
  \item $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$
  \end{itemize}
\item \warning{Zero covariance $\neq$ independence (except for normal)}
\item \note{For normal: independent $\Leftrightarrow \rho = 0$}
\end{itemize}

\subsection*{4.4 Covariance and Correlation}
\begin{itemize}[leftmargin=*]
\item \concept{Covariance:} \formula{\text{Cov}(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] = E[XY] - E[X]E[Y]}
\item \concept{Correlation:} \formula{\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}, \quad -1 \leq \rho \leq 1}
\item \concept{Properties:}
  \begin{itemize}
  \item $\text{Cov}(X,X) = \text{Var}(X)$
  \item $\text{Cov}(aX+b, cY+d) = ac \cdot \text{Cov}(X,Y)$
  \item $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)$
  \end{itemize}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Find $E[X]$, $E[Y]$, $E[XY]$
  \item Apply covariance formula
  \item For correlation, also find $\sigma_X$, $\sigma_Y$
  \end{enumerate}
\item \concept{Example:} HW4: Joint PDF, find $\rho$
\item \finance{Portfolio variance uses covariance matrix}
\end{itemize}

\subsection*{4.5 Bivariate Normal Distribution} \postmidterm{Critical!}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} $(X,Y)$ jointly normal with parameters $\mu_X, \mu_Y, \sigma_X^2, \sigma_Y^2, \rho$
\item \concept{Properties:}
  \begin{itemize}
  \item Linear combinations are normal
  \item Marginals are normal: $X \sim N(\mu_X, \sigma_X^2)$
  \item Independence $\Leftrightarrow \rho = 0$ (unique to normal!)
  \item Conditional is normal: $X|Y=y \sim N(\mu_{X|Y}, \sigma_{X|Y}^2)$
  \end{itemize}
\item \concept{Conditional Mean:} \formula{\mu_{X|Y} = \mu_X + \rho\frac{\sigma_X}{\sigma_Y}(y - \mu_Y)}
\item \concept{Conditional Variance:} \formula{\sigma_{X|Y}^2 = \sigma_X^2(1-\rho^2)}
\item \concept{Example:} HW5 Problem 1: Find $P(X+Y > 0)$
\item \note{$aX + bY$ is normal with specific mean/variance}
\end{itemize}

\subsection*{4.6 Transformations of Random Variables} \postmidterm{Complex!}
\begin{itemize}[leftmargin=*]
\item \concept{Single Variable:} $Y = g(X)$
  \begin{itemize}
  \item CDF Method: Find $F_Y(y) = P(g(X) \leq y)$
  \item PDF Method: $f_Y(y) = f_X(g^{-1}(y))|dg^{-1}/dy|$
  \end{itemize}
\item \concept{Jacobian Method:} $(U,V) = g(X,Y)$
  \formula{f_{UV}(u,v) = f_{XY}(x(u,v), y(u,v)) \cdot |J|}
  where \formula{J = \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix}}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Define transformation
  \item Find inverse transformation
  \item Compute Jacobian determinant
  \item Apply formula with absolute value
  \item Check new bounds
  \end{enumerate}
\item \warning{Don't forget absolute value of Jacobian!}
\item \concept{Example:} Polar coordinates: $X = R\cos\Theta$, $Y = R\sin\Theta$
\end{itemize}

\subsection*{4.7 Order Statistics}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} $X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$
\item \concept{Maximum:} \formula{F_{X_{(n)}}(x) = [F(x)]^n}
\item \concept{Minimum:} \formula{F_{X_{(1)}}(x) = 1 - [1-F(x)]^n}
\item \concept{PDF of $k$-th order statistic:}
  \formula{f_{X_{(k)}}(x) = \frac{n!}{(k-1)!(n-k)!}[F(x)]^{k-1}[1-F(x)]^{n-k}f(x)}
\item \concept{Example:} Max of 3 uniform(0,1) variables
\item \note{Range = $X_{(n)} - X_{(1)}$}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{5. MOMENT GENERATING FUNCTIONS}} \postmidterm{Post-M2}
% ============================================================

\subsection*{5.1 Definition and Properties}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} \formula{M_X(t) = E[e^{tX}] = \begin{cases} \sum e^{tx}p(x) & \text{discrete} \\ \int e^{tx}f(x)dx & \text{continuous} \end{cases}}
\item \concept{Moments:} \formula{E[X^k] = M^{(k)}(0)} (k-th derivative at 0)
\item \concept{Properties:}
  \begin{itemize}
  \item Uniqueness: Same MGF $\Rightarrow$ same distribution
  \item Linear: $M_{aX+b}(t) = e^{bt}M_X(at)$
  \item Sum of independent: $M_{X+Y}(t) = M_X(t) \cdot M_Y(t)$
  \end{itemize}
\item \concept{Common MGFs:}
  \begin{itemize}
  \item Binomial: $(1-p+pe^t)^n$
  \item Poisson: $e^{\lambda(e^t-1)}$
  \item Normal: $e^{\mu t + \sigma^2 t^2/2}$
  \item Exponential: $\lambda/(\lambda-t)$, $t < \lambda$
  \end{itemize}
\item \concept{Example:} Find distribution of sum using MGFs
\item \note{Not all distributions have MGF (e.g., Cauchy)}
\end{itemize}

\subsection*{5.2 Using MGFs for Sums}
\begin{itemize}[leftmargin=*]
\item \concept{Method:} For independent $X_1, ..., X_n$:
  \begin{enumerate}
  \item Find individual MGFs: $M_{X_i}(t)$
  \item Multiply: $M_S(t) = \prod M_{X_i}(t)$
  \item Match with known MGF to identify distribution
  \end{enumerate}
\item \concept{Example Applications:}
  \begin{itemize}
  \item Sum of normals is normal
  \item Sum of Poissons is Poisson
  \item Sum of gammas (same $\lambda$) is gamma
  \end{itemize}
\item \concept{Example:} $X_i \sim \text{Exp}(\lambda)$ independent, then $\sum X_i \sim \text{Gamma}(n, \lambda)$
\item \note{Powerful for proving CLT}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{6. LIMIT THEOREMS}} \postmidterm{Critical for Final!}
% ============================================================

\subsection*{6.1 Central Limit Theorem (CLT)} \postmidterm{Most Important!}
\begin{itemize}[leftmargin=*]
\item \concept{Statement:} If $X_1, ..., X_n$ are iid with mean $\mu$, variance $\sigma^2$:
  \formula{\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} N(0,1) \text{ as } n \to \infty}
\item \concept{Equivalent:} \formula{\bar{X}_n \approx N\left(\mu, \frac{\sigma^2}{n}\right) \text{ for large } n}
\item \concept{When to Use:}
  \begin{itemize}
  \item Large sample size (typically $n \geq 30$)
  \item Sum or average of many random variables
  \item Approximating discrete by continuous
  \end{itemize}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Verify conditions (iid, finite variance)
  \item Identify $\mu = E[X_i]$, $\sigma^2 = \text{Var}(X_i)$
  \item Standardize: $Z = (\bar{X} - \mu)/(\sigma/\sqrt{n})$
  \item Use normal table
  \end{enumerate}
\item \concept{Example:} 400 games, win \$3 with $p=0.25$, find $P(\text{total} > 240)$
\item \warning{Apply continuity correction for discrete!}
\end{itemize}

\subsection*{6.2 Normal Approximations}
\begin{itemize}[leftmargin=*]
\item \concept{Binomial Approximation:} If $X \sim \text{Binomial}(n,p)$ with $np(1-p) > 10$:
  \formula{X \approx N(np, np(1-p))}
\item \concept{Poisson Approximation:} If $X \sim \text{Poisson}(\lambda)$ with $\lambda > 30$:
  \formula{X \approx N(\lambda, \lambda)}
\item \concept{Continuity Correction:} For discrete $X$:
  \begin{itemize}
  \item $P(X = k) \approx P(k-0.5 < Y < k+0.5)$
  \item $P(X \leq k) \approx P(Y < k+0.5)$
  \item $P(X < k) \approx P(Y < k-0.5)$
  \end{itemize}
\item \concept{Example:} Binomial(100, 0.3), find $P(X > 35)$
\item \note{Correction improves accuracy significantly}
\end{itemize}

\subsection*{6.3 Law of Large Numbers (LLN)}
\begin{itemize}[leftmargin=*]
\item \concept{Weak LLN:} $\bar{X}_n \xrightarrow{P} \mu$ (convergence in probability)
\item \concept{Strong LLN:} $\bar{X}_n \xrightarrow{a.s.} \mu$ (almost sure convergence)
\item \concept{Interpretation:} Sample mean converges to true mean
\item \concept{Conditions:}
  \begin{itemize}
  \item Weak: Finite mean, pairwise uncorrelated
  \item Strong: Finite mean (iid case)
  \end{itemize}
\item \concept{Example:} Casino games, long-run frequency
\item \note{Foundation for frequentist probability}
\end{itemize}

\subsection*{6.4 Confidence Intervals}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} Interval estimate with specified confidence level
\item \concept{For Mean (known $\sigma$):} \formula{\bar{X} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}}
\item \concept{Common Values:}
  \begin{itemize}
  \item 90\% CI: $z_{0.05} = 1.645$
  \item 95\% CI: $z_{0.025} = 1.96$
  \item 99\% CI: $z_{0.005} = 2.576$
  \end{itemize}
\item \concept{Interpretation:} 95\% of such intervals contain true parameter
\item \concept{Width:} $2 \cdot z_{\alpha/2} \cdot \sigma/\sqrt{n}$
\item \warning{NOT "95\% chance parameter is in interval"}
\item \note{Larger $n$ $\Rightarrow$ narrower interval}
\end{itemize}

% ============================================================
\section*{\textcolor{sectioncolor}{7. SPECIAL TOPICS \& APPLICATIONS}} \postmidterm{Post-M2}
% ============================================================

\subsection*{7.1 Conditional Expectation} \postmidterm{Conceptual!}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:}
  \begin{itemize}
  \item Discrete: $E[X|Y=y] = \sum x \cdot P(X=x|Y=y)$
  \item Continuous: $E[X|Y=y] = \int x \cdot f_{X|Y}(x|y)dx$
  \end{itemize}
\item \concept{Law of Total Expectation:} \formula{E[X] = E[E[X|Y]]}
\item \concept{Properties:}
  \begin{itemize}
  \item Linearity: $E[aX+bZ|Y] = aE[X|Y] + bE[Z|Y]$
  \item Taking out known: $E[h(Y)X|Y] = h(Y)E[X|Y]$
  \item Independence: $E[X|Y] = E[X]$ if independent
  \end{itemize}
\item \concept{Law of Total Variance:} \formula{\text{Var}(X) = E[\text{Var}(X|Y)] + \text{Var}(E[X|Y])}
\item \concept{Example:} Breaking sticks problem
\item \note{$E[X|Y]$ is a function of $Y$, not a number!}
\end{itemize}

\subsection*{7.2 Bayesian Statistics} \postmidterm{Professor's Favorite!}
\begin{itemize}[leftmargin=*]
\item \concept{Bayesian Framework:}
  \formula{\pi(\theta|x) = \frac{L(x|\theta)\pi(\theta)}{\int L(x|\theta)\pi(\theta)d\theta}}
  where Prior $\times$ Likelihood $\rightarrow$ Posterior
\item \concept{Conjugate Priors:}
  \begin{itemize}
  \item Beta-Binomial: $\text{Beta}(\alpha,\beta) \rightarrow \text{Beta}(\alpha+x, \beta+n-x)$
  \item Gamma-Poisson: $\text{Gamma}(\alpha,\beta) \rightarrow \text{Gamma}(\alpha+\sum x_i, \beta+n)$
  \item Normal-Normal: With known variance
  \end{itemize}
\item \concept{Solution Steps:}
  \begin{enumerate}
  \item Specify prior $\pi(\theta)$
  \item Write likelihood $L(x|\theta)$
  \item Compute posterior (use conjugacy if possible)
  \item Normalize if needed
  \end{enumerate}
\item \concept{Example:} HW6 Monty Hall Bayesian analysis
\item \finance{Used in risk assessment, portfolio optimization}
\end{itemize}

\subsection*{7.3 Lognormal Distribution} \postmidterm{Finance Applications!}
\begin{itemize}[leftmargin=*]
\item \concept{Definition:} $Y = e^X$ where $X \sim N(\mu, \sigma^2)$
\item \concept{Properties:}
  \begin{itemize}
  \item Always positive (good for prices)
  \item Right-skewed
  \item Mean: $E[Y] = e^{\mu + \sigma^2/2}$
  \item Variance: $\text{Var}(Y) = e^{2\mu+\sigma^2}(e^{\sigma^2}-1)$
  \item Median: $e^\mu$
  \end{itemize}
\item \concept{Stock Price Model:} $S_t = S_0 \exp(X_t)$ where $X_t \sim N(\mu t, \sigma^2 t)$
\item \concept{Example:} HW5 Problem 2, Practice Final stock problems
\item \finance{Black-Scholes model foundation}
\item \note{Log returns are normal, prices are lognormal}
\end{itemize}

\subsection*{7.4 Additional Important Concepts}
\begin{itemize}[leftmargin=*]
\item \concept{Indicator Random Variables:}
  \begin{itemize}
  \item $I_A = 1$ if $A$ occurs, 0 otherwise
  \item $E[I_A] = P(A)$
  \item Useful for counting: $\sum I_{A_i}$
  \end{itemize}
\item \concept{Jensen's Inequality:} For convex $g$: $E[g(X)] \geq g(E[X])$
\item \concept{Chebyshev's Inequality:} $P(|X-\mu| \geq k\sigma) \leq 1/k^2$
\item \concept{Probability Integral Transform:} $F(X) \sim \text{Uniform}(0,1)$
\end{itemize}

% Placeholder for Section 8 and Appendices
\vspace{0.5cm}
\textit{[Section 8 (Practice Problems) and Appendices will be added in subsequent phases...]}

\end{multicols}
\end{document}
