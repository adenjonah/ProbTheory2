\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{mathrsfs}

\geometry{margin=1in}

\begin{document}

\begin{center}
{\Large \textbf{Homework 6 // CLT and Bayesian Statistics}}
\end{center}

\bigskip

\noindent
\textbf{Problem 1.} (Monty Hall: Sober and dizzy)

\medskip

\noindent
The Monty Hall problem: Monty hosts a game show. There are three doors: one hides a car and two hide goats. The contestant picks a door, which is not opened. Monty then opens another door which has a goat behind it. Finally, contestant must decide whether to stay with her original choice or switch to the other unopened door. The problem asks which is the better strategy: staying or switching?

\medskip

\noindent
To be precise, let’s label the door that contestant picks by $A$, and the other two doors by $B$ and $C$. Hypothesis $H_A$ is that the car is behind door $A$, and similarly for hypotheses $H_B$ and $H_C$.

\medskip

\noindent
(a) In the usual formulation, Monty is sober and knows the locations of the car and goats. So if the contestant picks a door with a goat, Monty always opens the other door with a goat. And if the contestant picks the door with a car, Monty opens one of the other two doors at random. Suppose that sober Monty Hall opens door B, revealing a goat. So the data is: “Monty showed a goat behind B”. Our hypotheses are “the car is behind door A”, etc. Make a Bayes table with prior, likelihood and posterior. Use the posterior probabilities to determine the best strategy.

\medskip

\noindent
(b) Now suppose that Monty is dizzy, i.e. he has completely forgotten where the car is and is only aware enough to randomly open one of the two doors not chosen by the contestant. It’s entirely possible he might accidentally reveal the car, ruining the show.

\medskip

\noindent
Dizzy Monty Hall opens door B, revealing a goat. Make a Bayes table with prior, likelihood and posterior. Use the posterior probabilities to determine the best strategy. (Hint: the data is the same but the likelihood function is not.)

\medskip

\noindent
(c) Based on Monty’s pre-show behavior, we think that Monty is sober with probability $0.7$ and dizzy with probability $0.3$. Repeat the analysis from parts (a) and (b) in this situation.

\bigskip
\bigskip

\noindent
\textbf{Problem 2.} Prediction

\medskip

\noindent
We are going to explore the dice problem from class further. I have five dice ($4$, $6$, $8$, $12$, or $20$ sides) and pick one at random (uniform probability). I then roll this die $n$ times and tell you that, miraculously, every roll resulted in the value $7$.

\medskip

\noindent
(a) First, consider just the first roll. Find the prior predictive probability that the first roll will be a $7$ and the posterior (after the first roll) predictive probability that the second roll will be a $7$. Also find the posterior (after the first roll) probabilities for the chosen die.

\medskip

\noindent
(b) Find the posterior probability $P(H \mid \text{data})$ for each die given the data of all $n$ rolls (your answers should involve $n$). What is the limit of each of these probabilities as $n$ grows to infinity? Explain why this makes sense.

\medskip

\noindent
(c) Given that my first 10 rolls resulted in $7$ (i.e., $n=10$), rank the possible values for my next roll from most likely to least likely. Note any ties in rank and explain your reasoning carefully. You need not do any computations to solve this problem.

\medskip

\noindent
(d) Let $x_i$ be the result of the $i$th roll. Find the posterior predictive pmf for the $(n+1)$st roll given the data. That is, find
\[
P(x_{n+1} \mid x_1 = 7, \ldots, x_n = 7)
\]
for $x_{n+1} = 1, \ldots, 20$. (Hint: use part (b) and the law of total probability. Many values of the pmf coincide, so you do not need to do 20 separate computations. You should check that your answer is consistent with your ranking in part (c) for $n=10$.)

\medskip

\noindent
(e) What function does the pmf in part (d) converge to as $n$ grows to infinity? Explain why this makes sense.

\bigskip
\bigskip

\noindent
\textbf{Problem 3. [CLT chapter 6]} Suppose that a random sample of size $n$ is to be taken from a distribution for which the mean is $\mu$ and the standard deviation is $3$. Use the central limit theorem to determine approximately the smallest value of $n$ for which the following relation will be satisfied:
\[
\Pr(|\bar X_n - \mu| < 0.3) \ge 0.95.
\]

\bigskip

\noindent
\textbf{Problem 4. [CLT chapter 6]} Suppose that 16 digits are chosen at random with replacement from the set $\{0,\ldots,9\}$. What is the probability that their average will lie between $4$ and $6$?

\bigskip

\noindent
\textbf{Problem 5. [Bayes]} Suppose that the proportion $\theta$ of defective items in a large manufactured lot is unknown, and the prior distribution of $\theta$ is the uniform distribution on the interval $[0,1]$. When eight items are selected at random from the lot, it is found that exactly three of them are defective. Determine the posterior distribution of $\theta$. Compute the expected mean and variance of $\theta$.

\bigskip

\noindent
\textbf{Problem 6. [Bayes]} Consider again the problem described in Problem 5, but suppose now that the prior p.d.f. of $\theta$ is as follows:
\[
\xi(\theta) =
\begin{cases}
2(1-\theta), & 0 < \theta < 1, \\
0, & \text{otherwise}.
\end{cases}
\]
As in previous Exercise, suppose that in a random sample of eight items exactly three are found to be defective. Determine the posterior distribution of $\theta$.

\end{document}